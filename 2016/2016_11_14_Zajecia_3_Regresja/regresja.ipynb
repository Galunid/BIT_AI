{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# magiczna linijka, żeby wykresy się rysowały bezpośrednio w dokumencie\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regresja liniowa\n",
    "\n",
    "<!-- -->1. W pliku __ex1.csv__ znajdują się dane, których będziemy używać w dalszej części ćwiczeń. Pierwsza kolumna to losowa liczba, druga wartość oznacza jak bardzo prowadzący ją lubi. Estymować będziemy lubienie przez prowadzącego. Na początek dane należy wczytać. Można użyć biblioteki _pandas_, można to zrobić bezpośrednio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Wczytywanie\n",
    "# ...\n",
    "\n",
    "# Na końcu upewnijmy się, że bawimy się z numpyowymi wektorami \n",
    "N = len(X)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "X = X.reshape((N, 1))\n",
    "Y = Y.reshape((N, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- -->2. Mając już wczytane dane warto na nie popatrzeć czujnym okiem. Przy pomocy biblioteki matplotlib narysuj wykres z wcześniej wczytanymi danymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# coś tam, coś tam\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- --> 3. Przejdźmy do meritum. Zaimplementujmy szukanie regresji liniowej metodą najmniejszych kwadratów. Dla ułatwienia przyjmijmy, że operujemy na płaszczyźnie dwuwymiarowej, więc rozważamy rozwiązania postaci: $$\\theta_0 + \\theta_1 x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(theta, x, y):\n",
    "    # ...\n",
    "    pass\n",
    "\n",
    "def gradient_step(theta, x, y, alpha):\n",
    "    new_theta = []\n",
    "    # ...\n",
    "    return new_theta\n",
    "    \n",
    "def gradient_descent(theta, x, y, alpha, steps):\n",
    "    for i in range(steps):\n",
    "        if i%100 == 0:\n",
    "            print \"Current cost: \", cost(theta, x, y)\n",
    "        theta = gradient_step(theta, x, y, alpha)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- --> 4. Sprawdź swój kod w boju! \n",
    "1. Podziel dane na zbiór uczący i testowy (np. w stosunku 3:1). Pamiętaj, że na początku dane warto losowo wymieszać. Przy pomocy napisanych wcześniej funkcji wytrenuj model i porównaj wartość błędu dla obu zbiorów. Jak zmieniają się wyniki w zależności od parametrów alpha i liczby iteracji?\n",
    "2. Na jednym wykresie pokaż zarówno dane wejściowe jak i znalezioną funkcję.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros((2, 1))\n",
    "\n",
    "x_train = # ...\n",
    "y_train = # ...\n",
    "\n",
    "\n",
    "x_test = # ...\n",
    "y_test = # ...\n",
    "\n",
    "theta = gradient_descent(initial_theta, x_train, y_train, 0.1, 1000)\n",
    "\n",
    "print \"Training cost: \", cost(x_train, y_train)\n",
    "print \"Test cost: \", cost(x_test, y_test)\n",
    "\n",
    "\n",
    "# plot all the data!\n",
    "# ...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- --> 5. Uogólnij swój kod tak, aby działał niezależnie od liczby featurów. Dodatkowe punkty uzyskasz, jeśli operacje będą zwektoryzowane przy pomocy biblioteki _numpy_. (tj. zamiast pętli będą operacje na _numpy.array_). Sprawdź, czy wyniki wciąż są satysfakcjonujące wykonując ponownie zadanbie __4__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- --> 6. Na koniec postarajmy się zrobić to lepiej. Na potrzeby trenowania modelu wzbogać zbiór featurów o dodatkową kolumnę - kwadrat wartości. Porównaj wartości błędów i wykresy dla zwykłych danych i tych wzbogaconych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros((3, 1))\n",
    "\n",
    "x_train = # ...\n",
    "y_train = # ...\n",
    "\n",
    "\n",
    "x_test = # ...\n",
    "y_test = # ...\n",
    "\n",
    "theta = gradient_descent(initial_theta, x_train, y_train, 0.1, 1000)\n",
    "\n",
    "print \"Training cost: \", cost(x_train, y_train)\n",
    "print \"Test cost: \", cost(x_test, y_test)\n",
    "\n",
    "\n",
    "# plot all the data!\n",
    "# ...\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
