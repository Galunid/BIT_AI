{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for text generation - character models\n",
    "\n",
    "The first kind of models we'll work with today are character models. The LSTM will be fed raw text, character by character. It will then try to predict what the next character will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data we want to play with. By default, you can play with a sample from Shakespeare's works as gathered by Andrej Karpathy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget -O data/tadeusz.txt https://wolnelektury.pl/media/book/txt/pan-tadeusz.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tadeusz.txt') as f:\n",
    "    lines_txt = f.readlines()\n",
    "lines_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split it into single characters, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_txt = []\n",
    "for line in lines_txt:\n",
    "    chars_txt.extend(line[:])\n",
    "chars_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to translate the characters into something interpretable by a neural network. We'll use one-hot vectors for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frst, we'll create a set of all available characters\n",
    "chars = sorted(list(set(chars_txt)))\n",
    "# then, mappings of characters to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "char_to_onehot = {}\n",
    "for i, c in int_to_char.items():\n",
    "    char_to_onehot[c] = np.zeros(len(chars))\n",
    "    char_to_onehot[c][i] = 1\n",
    "\n",
    "len(chars), len(chars_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data will be character sequences. As the model has to predict the next character in a sequence, the target data will be just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "X_l = []\n",
    "Y_l = []\n",
    "for i in range(0, len(chars_txt) - sequence_length, 1):\n",
    "\tseq_in = chars_txt[i:i + sequence_length]\n",
    "\tseq_out = chars_txt[i + sequence_length]\n",
    "\tX_l.append([char_to_onehot[char] for char in seq_in])\n",
    "\tY_l.append(char_to_int[seq_out])\n",
    "len(X_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to transform the data into PyTorch Variables, we'll transform it into np.arrays first. Notice we only use 100k first sequences. Too many sequences present a risk of memory errors!\n",
    "\n",
    "We'll also split the data into training and test sets here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(X_l[:100000], (-1, sequence_length, len(chars)))\n",
    "# X = X / len(chars)\n",
    "\n",
    "Y = np.array(Y_l)\n",
    "data_size = X.shape[0]\n",
    "idx = np.arange(data_size)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(idx)\n",
    "train_size = int(data_size * 0.7)\n",
    "# test_size = int(data_size * 0.03)\n",
    "\n",
    "train_idx = idx[:train_size]\n",
    "test_idx = idx[train_size:]\n",
    "\n",
    "X_train = X[train_idx]\n",
    "Y_train = Y[train_idx]\n",
    "\n",
    "X_test = X[test_idx]\n",
    "Y_test = Y[test_idx]\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's wrap the data into PyTorch Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_var = Variable(torch.Tensor(X_train).type(dtype))\n",
    "Y_train_var = Variable(torch.Tensor(Y_train).type(dtype).long())\n",
    "\n",
    "X_test_var = Variable(torch.Tensor(X_test).type(dtype))\n",
    "Y_test_var = Variable(torch.Tensor(Y_test).type(dtype).long())\n",
    "\n",
    "X_train_var.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define the model. It will be a very shallow network consisting of just 3 layers. It's enough for our purpose, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, lstm_layers_no=3, vocab_size=len(chars)):\n",
    "        super(CharacterModel, self).__init__()\n",
    "        self.lstm_layers_no = lstm_layers_no\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_layer = nn.LSTM(vocab_size, hidden_dim, lstm_layers_no, dropout=0.2)\n",
    "        self.dropout_layer = nn.Dropout(0.2)\n",
    "        self.vec2token = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        self.hidden = (Variable(torch.zeros(self.lstm_layers_no, batch_size, self.hidden_dim).type(dtype)),\n",
    "                Variable(torch.zeros(self.lstm_layers_no, batch_size, self.hidden_dim).type(dtype)))\n",
    "        return self.hidden\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        lstm_input = sequence.permute(1, 0, 2)\n",
    "        lstm_out, self.hidden = self.lstm_layer(lstm_input, self.hidden)        \n",
    "        tags = self.vec2token(self.dropout_layer(self.hidden[0][self.lstm_layers_no-1]))\n",
    "        return tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the model, we can initialize it. Feel free to play with the hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "lstm_layers_no = 3\n",
    "\n",
    "model = CharacterModel(hidden_dim, lstm_layers_no=lstm_layers_no).type(dtype)\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "epochs_no = 100\n",
    "\n",
    "loss_history = []\n",
    "for epoch in range(epochs_no):\n",
    "    model.train()\n",
    "    train_losses_l = []\n",
    "    for i in range(100):\n",
    "\n",
    "        model.zero_grad()\n",
    "        model.init_hidden(batch_size)\n",
    "        \n",
    "        idx = torch.Tensor(np.random.randint(X_train_var.size()[0], size=batch_size)).type(dtype).long()\n",
    "        sequence_in = X_train_var[idx]\n",
    "        \n",
    "        targets = Y_train_var[idx]\n",
    "        tag_scores = model(sequence_in)\n",
    "        loss = loss_fun(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses_l.append(loss.data.cpu().numpy()[0])\n",
    "        \n",
    "    model.eval()\n",
    "    model.init_hidden(test_batch_size)\n",
    "    \n",
    "    test_idx = torch.Tensor(np.random.randint(X_test_var.size()[0], size=test_batch_size)).type(dtype).long()\n",
    "    test_sequence_in = X_test_var[test_idx]\n",
    "    test_targets = Y_test_var[test_idx]\n",
    "    test_tag_scores = model(test_sequence_in)\n",
    "    test_loss = loss_fun(test_tag_scores, test_targets).data.cpu().numpy().sum()\n",
    "    train_losses = np.array(train_losses_l)\n",
    "\n",
    "    loss_history.append((train_losses.mean(), test_loss))\n",
    "\n",
    "    print(epoch, loss_history[-1] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the losses decreasing is one thing. We know the model gets *something* more and more accurately. However, in case of the RNN's there is a simple, cool way to visualize that *something* for oneself!\n",
    "\n",
    "We can see the model in action by sampling from it. The model will make a prediction beased on some starting sequence. We'll then 'cut off' the first element of the sequence and append the prediction to the it. Then we repeat the process and generate as much text as we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_model(seq_in):\n",
    "    seq_var = Variable(torch.Tensor(seq_in).type(dtype)) \n",
    "    out = model(seq_var)\n",
    "    probs = nn.functional.softmax(out, dim=1).data.cpu().numpy()[0]\n",
    "    # to make things less deterministic, instead of taking the character with the highest probability, \n",
    "    # we'll sample from all characters with the probability distribution taken from network's predictions\n",
    "    chosen = np.random.choice(np.arange(probs.shape[0]), p=probs)\n",
    "    return int(chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(start_seq, seq_len=1000):\n",
    "    model.hidden = model.init_hidden()\n",
    "    sys.stdout.write(start_seq)\n",
    "    seq = [[char_to_onehot[t] for t in start_seq]]\n",
    "    \n",
    "    for _ in range(seq_len):\n",
    "        next_int = sample_from_model(seq)\n",
    "        next_token = int_to_char[next_int]\n",
    "        sys.stdout.write(next_token)\n",
    "        seq = [seq[0][1:] + [char_to_onehot[int_to_char[next_int]]]]\n",
    "        if len(seq[0]) > sequence_length:\n",
    "            seq = seq[0, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want the play to start with me saying something:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_sequence = 'Litwo!\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "generate(start_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
