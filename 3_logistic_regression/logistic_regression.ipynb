{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hi!\n",
    "In today's workshop we are going to learn about most known concept of supervised learning which is **classification**.\n",
    "\n",
    "### What is classification?\n",
    "Classification is a problem of predicting discrete value (classes) for given features. It is mainly viewed as a supervised learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import solutions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like last time, we'll work with a very real-world dataset describing a couple hundred cases of breast cancer, which presents an example of a case for **binary classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(load_breast_cancer().DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll split our data int train, test, and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, train_size=0.7)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, train_size=0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about applying linear regression for classification?\n",
    "\n",
    "Let's take a look at the target data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bunch of ones and zeros! Wouldn't it make sense to just train a linear regressor on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret these predictions? Maybe we need something different?\n",
    "\n",
    "![classification_regression](img/clas_reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is about applying a \"squashing\" function to the hypotheses when calculating loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$h_w(x) = \\sum_{j=0}^k w_j x_j = wx$$\n",
    "\n",
    "### $$\\hat{y} = \\sigma(h_w(x))$$ \n",
    "\n",
    "### One of such squashing functions is sigmoid function:\n",
    "### $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, sigmoid(x))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(np.inf), sigmoid(-np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because of non-linearities in our hypotheses, we also need to update our loss function.\n",
    "\n",
    "We'll use a logarythmic loss function which quite nicely captures an intuition, that we want the predictions datapoins which should be predicted as $0$ as close to $0$ as possible, and, analogically, predictions which should be $1$, as close to $1$ as possible:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ L(w) = \\frac{-1}{n}(\\sum_{i=0}^n y^{(i)}\\log{h_w(x^{(i)})} + (1-y^{(i)})\\log{(1-h_w(x^{(i)}))} )$$\n",
    "\n",
    "### $$ y^{(i)} \\in \\{0, 1\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 0\n",
    "x = np.linspace(0, 0.9999, 1000)\n",
    "plt.plot(x, -np.log(1 - x))\n",
    "plt.ylim(-1, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 1\n",
    "x = np.linspace(0.0001, 1, 1000)\n",
    "plt.plot(x, -np.log(x))\n",
    "plt.ylim(-1, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and implement this new loss function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "    W: np.ndarray, \n",
    "    X: np.ndarray, \n",
    "    Y: np.ndarray, \n",
    "    eps: float = 0.01 # the epsilon parameter is for numeric stability of logarithm\n",
    ") -> float:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = solutions.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(X.shape[1])\n",
    "print(loss(W, X, y, eps=0.1))\n",
    "print(solutions.loss(W, X, y, eps=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about gradient descent procedure? How does it change? Let's derive the gradient!\n",
    "\n",
    "[we'll do that on the board] \n",
    "It turns out, it's very simple!\n",
    "\n",
    "### $$\n",
    "\\frac{\\partial L(W)}{\\partial W} =\\frac{1}{n}(\\sum_{i=0}^n x^{(i)} \\cdot (h_w(x^{(i)}) - y^{(i)}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(\n",
    "    W, \n",
    "    X, \n",
    "    Y,\n",
    "    learning_rate=0.01\n",
    ") -> np.ndarray:\n",
    "    return np.zeros_like(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_step = solutions.gradient_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(X.shape[1])\n",
    "\n",
    "yours = gradient_step(W, X, y, learning_rate=0.1)\n",
    "provided = solutions.gradient_step(W, X, y, learning_rate=0.1)\n",
    "print(yours - provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not forget about adding the bias feature and normalizing the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_feature(X):\n",
    "       return np.c_[np.ones(len(X)), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = add_bias_feature(X_train)\n",
    "X_val = add_bias_feature(X_val)\n",
    "X_test = add_bias_feature(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, *norm_parameters = solutions.std_normalization(X_train)\n",
    "X_val, *_ = solutions.std_normalization(X_val, *norm_parameters)\n",
    "X_test, *_ = solutions.std_normalization(X_test, *norm_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "W = np.random.randn(X_train.shape[1])\n",
    "train_costs = []\n",
    "val_costs = []\n",
    "train_steps = 100\n",
    "for _ in range(train_steps):\n",
    "    train_costs.append(loss(W, X_train, y_train, eps=0.001))\n",
    "    val_costs.append(loss(W, X_val, y_val, eps=0.001))\n",
    "    W = gradient_step(W, X_train, y_train, learning_rate=0.1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(train_steps), train_costs)\n",
    "plt.plot(np.arange(train_steps), val_costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, solutions._hypotheses(W, X_train) >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_val, solutions._hypotheses(W, X_val) >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression(C=10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A great score! Or is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_ind = np.argwhere(y_val == 1).reshape(-1)\n",
    "negative_ind = np.argwhere(y_val == 0).reshape(-1)\n",
    "X_val_pos = X_val[positive_ind]\n",
    "y_val_pos = y_val[positive_ind]\n",
    "X_val_neg = X_val[negative_ind]\n",
    "y_val_neg = y_val[negative_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_val_pos, solutions._hypotheses(W, X_val_pos) >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_val_neg, solutions._hypotheses(W, X_val_neg) >= 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve higher accuracies on positive examples, than on negative ones. In practice, this means we're likelier to classify tumors as malignant than not. \n",
    "\n",
    "Better safe than sorry? Turns out, not always. Can we dig deeper into the performance of our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall\n",
    "We can divide classifications of our model into four classes:\n",
    "\n",
    "| Predicted/Actual | 0   | 1   |\n",
    "|------------------|-----|-----|\n",
    "| 0                | True negative | False negative|\n",
    "| 1                | False positive | True positive | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy - a first intuition**\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{T_p + T_n}{T_n + T_p + F_n + F_p}\n",
    "$$\n",
    "\n",
    "However, as we've just seen, this metric may be deceiving (consider class imbalance!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out there is a more reliable way to measure the performance of our model:\n",
    "\n",
    "- **Precision** - *what fraction of our positive classifications is correct?*\n",
    "$$\n",
    "Precision = \\frac{T_p}{T_p + F_p}\n",
    "$$\n",
    "\n",
    "- **Recall** - *what fraction of actual positive examples has been classified correctly?*\n",
    "$$\n",
    "Recall = \\frac{T_p}{T_p + F_n}\n",
    "$$\n",
    "\n",
    "We want both of those values to be as high as possible (duh).\n",
    "\n",
    "However, sometimes we have to make a trade off between them and decide with our classification method that one will be higher and the other lower.\n",
    "\n",
    "A metric which nicely mixes the two above is called the **F1 score** - it's high when both precision and recall are high enough, but low when one of them is sacrificed for the sake of another.\n",
    "\n",
    "$$\n",
    "F1 = \\frac{2PR}{P +R}\n",
    "$$\n",
    "\n",
    "#### Can precision and recall be manipulated without tinkering with the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision_recall(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    W: np.ndarray,\n",
    "    threshold: float\n",
    "):\n",
    "    y_pred = solutions._hypotheses(W, X)\n",
    "    y_pred_bin = y_pred >= threshold\n",
    "    print('precision', precision_score(y, y_pred_bin))\n",
    "    print('recall', recall_score(y, y_pred_bin))\n",
    "    print('F1 score', f1_score(y, y_pred_bin))\n",
    "    positive_ind = np.argwhere(y == 1).reshape(-1)\n",
    "    negative_ind = np.argwhere(y == 0).reshape(-1)\n",
    "    y_pos = y[positive_ind]\n",
    "    y_neg = y[negative_ind]\n",
    "    y_pos_pred = y_pred_bin[positive_ind]\n",
    "    y_neg_pred = y_pred_bin[negative_ind]\n",
    "    print('total accuracy', accuracy_score(y, y_pred_bin))\n",
    "    print('positive accuracy', accuracy_score(y_pos, y_pos_pred))\n",
    "    print('negative accuracy', accuracy_score(y_neg, y_neg_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    calc_precision_recall,\n",
    "    X=fixed(X_val),\n",
    "    y=fixed(y_val),\n",
    "    W=fixed(W),\n",
    "    threshold=widgets.FloatSlider(\n",
    "        value=0.5,\n",
    "        min=0,\n",
    "        max=1,\n",
    "        step=0.01\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does F1 score depend on the threshhold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(.01, .99, 100)\n",
    "scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = solutions._hypotheses(W, X_val)\n",
    "    y_pred_bin = y_pred >= t\n",
    "    scores.append(f1_score(y_val, y_pred_bin))\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.plot(thresholds, scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To better visualize how precision recall depend on each other, we can also plot an AUROC curve\n",
    "\n",
    "**A**rea\n",
    "\n",
    "**U**nder\n",
    "\n",
    "**R**eceiver\n",
    "\n",
    "**O**perating\n",
    "\n",
    "**C**haracteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(.01, .99, 100)\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = solutions._hypotheses(W, X_val)\n",
    "    y_pred_bin = y_pred >= t\n",
    "    precisions.append(precision_score(y_val, y_pred_bin))\n",
    "    recalls.append(recall_score(y_val, y_pred_bin))\n",
    "plt.xlim(0, 1.2)\n",
    "plt.ylim(0, 1.2)\n",
    "plt.grid(True)\n",
    "plt.plot(precisions, recalls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bit_ai]",
   "language": "python",
   "name": "conda-env-bit_ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
