{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zr1BMxu8Rjkq"
   },
   "source": [
    "Accelerated computing is replacing CPU-only computing as best practice. The litany of breakthroughs driven by \n",
    "accelerated computing, the ever increasing demand for accelerated applications, programming conventions that ease \n",
    "writing them, and constant improvements in the hardware that supports them, are driving this inevitable transition.\n",
    "\n",
    "At the center of accelerated computing's success, both in terms of its impressive performance, and its ease of use, is the [CUDA](https://developer.nvidia.com/about-cuda) compute platform. CUDA provides a coding paradigm that extends languages like C, C++, Python, and Fortran, to be capable of running accelerated, massively parallelized code on the world's most performant parallel processors: NVIDIA GPUs. CUDA accelerates applications drastically with little effort, has an ecosystem of highly optimized libraries for [DNN](https://developer.nvidia.com/cudnn), [BLAS](https://developer.nvidia.com/cublas), [graph analytics](https://developer.nvidia.com/nvgraph), [FFT](https://developer.nvidia.com/cufft), and more, and also ships with powerful [command line](http://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview) and [visual profilers](http://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual).\n",
    "\n",
    "CUDA supports many, if not most, of the [world's most performant applications](https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/catalog/?product_category_id=58,59,60,293,98,172,223,227,228,265,487,488,114,389,220,258,461&search=) in, [Computational Fluid Dynamics](https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/catalog/?product_category_id=10,12,16,17,19,51,53,71,87,121,124,156,157,195,202,203,204,312,339,340,395,407,448,485,517,528,529,541,245,216,104,462,513,250,492,420,429,490,10,12,16,17,19,51,53,71,87,121,124,156,157,195,202,203,204,312,339,340,395,407,448,485,517,528,529,541,245,216,104,462,513,250,492,420,429,490,10,12,16,17,19,51,53,71,87,121,124,156,157,195,202,203,204,312,339,340,395,407,448,485,517,528,529,541,245,216,104,462,513,250,492,420,429,490&search=), [Molecular Dynamics](https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/catalog/?product_category_id=8,57,92,123,211,213,237,272,274,282,283,307,325,337,344,345,351,362,365,380,396,398,400,435,507,508,519,8,57,92,123,211,213,237,272,274,282,283,307,325,337,344,345,351,362,365,380,396,398,400,435,507,508,519,8,57,92,123,211,213,237,272,274,282,283,307,325,337,344,345,351,362,365,380,396,398,400,435,507,508,519,8,57,92,123,211,213,237,272,274,282,283,307,325,337,344,345,351,362,365,380,396,398,400,435,507,508,519&search=), [Quantum Chemistry](https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/catalog/?product_category_id=8,57,92,123,211,213,237,272,274,282,283,307,325,337,344,345,351,362,365,380,396,398,400,435,507,508,519,8,57,92,123,211,213,237,272,274,282,283,307,325,337,344,345,351,362,365,380,396,398,400,435,507,508,519&search=), [Physics](https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/catalog/?product_category_id=6,24,116,118,119,135,229,231,372,373,392,393,489,493,494,495,496,497,498,67,170,216,281,6,24,116,118,119,135,229,231,372,373,392,393,489,493,494,495,496,497,498,67,170,216,281,6,24,116,118,119,135,229,231,372,373,392,393,489,493,494,495,496,497,498,67,170,216,281,6,24,116,118,119,135,229,231,372,373,392,393,489,493,494,495,496,497,498,67,170,216,281,6,24,116,118,119,135,229,231,372,373,392,393,489,493,494,495,496,497,498,67,170,216,281&search=) and HPC.\n",
    "\n",
    "Learning CUDA will enable you to accelerate your own applications. Accelerated applications perform much faster than their CPU-only couterparts, and make possible computations that would be otherwise prohibited given the limited performance of CPU-only applications. In this lab you will receive an introduction to programming accelerated applications with CUDA C/C++, enough to be able to begin work accelerating your own CPU-only applications for performance gains, and for moving into novel computational territory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ye62EPubRjks"
   },
   "source": [
    "## Accelerated Systems\n",
    "\n",
    "*Accelerated systems*, also referred to as *heterogeneous systems*, are those composed of both CPUs(hosts) and GPUs(devices). Accelerated systems run CPU programs which in turn, launch functions that will benefit from the massive parallelism provided by GPUs. This lab environment is an accelerated system which includes an NVIDIA GPU. Information about this GPU can be queried with the `nvidia-smi` (*Systems Management Interface*) command line command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1734,
     "status": "ok",
     "timestamp": 1551031634876,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "ndFBNVcJRjku",
    "outputId": "8ff53062-9dfd-4e7e-f357-86167be79145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2018 NVIDIA Corporation\n",
      "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
      "Cuda compilation tools, release 10.0, V10.0.130\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQ1ExygHRjkx"
   },
   "source": [
    "![floating-point](https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/floating-point-operations-per-second.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x83pUY0XRjky"
   },
   "source": [
    "## Why does it happens?\n",
    "\n",
    "The reason behind the discrepancy in floating-point capability between the CPU and the GPU is that the GPU is specialized for compute-intensive, highly parallel computation - exactly what graphics rendering is about - and therefore designed such that more transistors are devoted to data processing rather than data caching and flow control, as illustrated below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtvbVsOuRjkz"
   },
   "source": [
    "![cpu-gpu](https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/gpu-devotes-more-transistors-to-data-processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drj5TZeiRjk0"
   },
   "source": [
    "More specifically, the GPU is especially well-suited to address problems that can be expressed as data-parallel computations - the same program is executed on many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic operations to memory operations. Because the same program is executed for each data element, there is a lower requirement for sophisticated flow control, and because it is executed on many data elements and has high arithmetic intensity, the memory access latency can be hidden with calculations instead of big data caches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ed1mhkD-Rjk1"
   },
   "source": [
    "## Writing first program\n",
    "\n",
    "Any cuda programm is consists of CPU and GPU code, and have `.cu` extension.\n",
    "GPU code is simply C functions, called `kernels`, declared with `__global__` keyword.\n",
    "\n",
    "## Kernel execution\n",
    "\n",
    "Kernels executes in threads which is grouped by blocks of specified size using some_kernel<<< num_blocks, num_threads_per_block >>> syntax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Bhwoq0CRjk1"
   },
   "source": [
    "# Writing first CUDA program\n",
    "\n",
    "Below code contains 2 C function that executes on CPU. Your goal is to refactor the `print_hello_GPU` function in the source file so that it actually runs on the GPU, and prints a message \"hello GPU\" 2 times(this time using 1 block). Fill free to look up [the solution](solutions/hello_solution.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1551031640137,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "ejuGmY9uRjk2",
    "outputId": "93677705-940e-4f47-fba4-079304c69417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "void print_hello_CPU(){\n",
    "    printf(\"hello from CPU!\\n\");\n",
    "}\n",
    "\n",
    "void print_hello_GPU(){\n",
    "    printf(\"hello\\n\");\n",
    "}\n",
    "\n",
    "int main(void){\n",
    "    print_hello_CPU();\n",
    "    \n",
    "    print_hello_GPU();\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2243,
     "status": "ok",
     "timestamp": 1551031644256,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "1L5D8TMSRjk6",
    "outputId": "f08ae817-c6bf-4dc6-9d90-8e849a52984e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello from CPU!\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o hello hello.cu --run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3U4h_rsRjk-"
   },
   "source": [
    "In this program you can see the `cudaDeviceSynchronize()` function, it is used to tell CPU to hold on and wait every kernel to finish work before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2te3sKYRjk-"
   },
   "source": [
    "## Performing different work in each thread\n",
    "\n",
    "Each thread that executes the kernel is given a unique `thread ID` that is accessible within the kernel through the built-in `threadIdx` variable.\n",
    "\n",
    "As was mentioned before, each thread placed in some `thread block` and blocks are organized into `grid` (see image below), and we can access the `block Id` and `block dimensions` using the built-in `blockIdx` and `blockDim` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilWYF13kRjk_"
   },
   "source": [
    "![grid-block-thread](https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/grid-of-thread-blocks.png)\n",
    "\n",
    "![SM-im](https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/automatic-scalability.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_1VfDQFRjlA"
   },
   "source": [
    "As you can see `grids` and `blocks` can be multidimensional(one-dimensional, two-dimensional, or three-dimensional), that's why `threadIdx`, `blockIdx`, `blockDim` and `gridDim`, is three-dimensional vector of built-in type dim3, each dimention can be accessed using `.x`, `.y`, `.z` for e.g `threadIdx.x`, `threadIdx.y`, `threadIdx.z` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-R5WnP2RjlA"
   },
   "source": [
    "For now lets look at one-dimensional example. Below you can see an example of kernel that add 2 vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1551031852112,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "gVlYbM-YRjlB",
    "outputId": "c731b2df-60de-4834-e790-001fabe2a1ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vecadd.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile vecadd.cu\n",
    "#include <stdio.h>\n",
    "#include <time.h> \n",
    "\n",
    "__global__ void VecAdd(float* A, float* B, float* C, int N){\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(i < N){\n",
    "        C[i] = A[i] + B[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void) {\n",
    "    \n",
    "    int N = 18000000;\n",
    "    size_t size = N*sizeof(float);\n",
    "    float* h_a = (float*) malloc(size);\n",
    "    float* h_b = (float*) malloc(size);\n",
    "    float* h_c = (float*) malloc(size);\n",
    "\n",
    "    for(int i = 0;i<N;i++){\n",
    "        h_a[i] = 1.5;\n",
    "        h_b[i] = 2.7;\n",
    "    }\n",
    "    \n",
    "    float* d_a;\n",
    "    cudaMalloc(&d_a, size);\n",
    "\n",
    "    float* d_b;\n",
    "    cudaMalloc(&d_b, size);\n",
    "\n",
    "    float* d_c;\n",
    "    cudaMalloc(&d_c, size);\n",
    "\n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    int threads_per_block = 256;\n",
    "    int blocks_per_grid = (N + threads_per_block -1)/threads_per_block;\n",
    "\n",
    "    VecAdd<<<blocks_per_grid, threads_per_block>>>(d_a, d_b, d_c, N);\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    for(int i = 0; i<5;i++){\n",
    "        printf(\"%f \", h_c[i]);\n",
    "    }\n",
    "\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    for(int i = N-1; i>N-6;i--){\n",
    "        printf(\"%f \", h_c[i]);\n",
    "    }\n",
    "\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "\n",
    "    free(h_a);\n",
    "    free(h_b);\n",
    "    free(h_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yO9zB7B3RjlE"
   },
   "source": [
    "You can see that we use `cudaMalloc`, `cudaMemcpy` and `cudaFree` functions, they are responsible for memory management. `cudaMemcpy` also takes one of three possible arguments (`cudaMemcpyHostToDevice`, `cudaMemcpyDeviceToHost`, `cudaMemcpyDeviceToDevice`) to specify \"direction of copying\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2738,
     "status": "ok",
     "timestamp": 1551031855542,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "u0woF1uERjlF",
    "outputId": "d3989541-6411-4084-a1f0-e775e12da9cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.200000 4.200000 4.200000 4.200000 4.200000 \n",
      "4.200000 4.200000 4.200000 4.200000 4.200000 \n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vecadd vecadd.cu --run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3676,
     "status": "ok",
     "timestamp": 1551031857389,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "JrK0kSpsRjlH",
    "outputId": "53e0fe34-11a5-431c-ffd6-9a9f546e3a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==908== NVPROF is profiling process 908, command: ./vecadd\n",
      "4.200000 4.200000 4.200000 4.200000 4.200000 \n",
      "4.200000 4.200000 4.200000 4.200000 4.200000 \n",
      "==908== Profiling application: ./vecadd\n",
      "==908== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   64.64%  53.448ms         1  53.448ms  53.448ms  53.448ms  [CUDA memcpy DtoH]\n",
      "                   33.29%  27.523ms         2  13.761ms  13.740ms  13.782ms  [CUDA memcpy HtoD]\n",
      "                    2.07%  1.7143ms         1  1.7143ms  1.7143ms  1.7143ms  VecAdd(float*, float*, float*, int)\n",
      "      API calls:   57.06%  138.06ms         3  46.019ms  225.27us  137.56ms  cudaMalloc\n",
      "                   34.13%  82.581ms         3  27.527ms  13.898ms  54.696ms  cudaMemcpy\n",
      "                    7.74%  18.723ms         3  6.2409ms  281.88us  9.2441ms  cudaFree\n",
      "                    0.71%  1.7276ms         2  863.79us  5.6620us  1.7219ms  cudaDeviceSynchronize\n",
      "                    0.13%  314.59us        96  3.2760us     160ns  141.58us  cuDeviceGetAttribute\n",
      "                    0.12%  292.42us         1  292.42us  292.42us  292.42us  cudaLaunchKernel\n",
      "                    0.09%  213.01us         1  213.01us  213.01us  213.01us  cuDeviceTotalMem\n",
      "                    0.01%  19.142us         1  19.142us  19.142us  19.142us  cuDeviceGetName\n",
      "                    0.00%  2.9340us         1  2.9340us  2.9340us  2.9340us  cuDeviceGetPCIBusId\n",
      "                    0.00%  1.7690us         3     589ns     168ns     913ns  cuDeviceGetCount\n",
      "                    0.00%  1.0700us         2     535ns     222ns     848ns  cuDeviceGet\n",
      "                    0.00%     308ns         1     308ns     308ns     308ns  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./vecadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1551031867290,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "H-EviURlRjlJ",
    "outputId": "2c328968-70f0-4885-d691-f9021677107a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vecadd.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile vecadd.c\n",
    "#include <stdio.h>\n",
    "#include <time.h> \n",
    "\n",
    "void VecAdd(float* A, float* B, float* C, int N){\n",
    "\t\n",
    "\tfor(int i=0; i < N; i++){\n",
    "\t\tC[i] = A[i] + B[i];\n",
    "\t}\n",
    "}\n",
    "\n",
    "int main(void) {\n",
    "    clock_t start,end;\n",
    "    \n",
    "    start = clock();\n",
    "    \n",
    "\tint N = 18000000;\n",
    "\tsize_t size = N*sizeof(float);\n",
    "\tfloat* h_a = (float*) malloc(size);\n",
    "\tfloat* h_b = (float*) malloc(size);\n",
    "\tfloat* h_c = (float*) malloc(size);\n",
    "\n",
    "\tfor(int i = 0;i<N;i++){\n",
    "\t\th_a[i] = 1.5;\n",
    "\t\th_b[i] = 2.7;\n",
    "\t}\n",
    "    \n",
    "    \n",
    "    \n",
    "\tVecAdd(h_a, h_b, h_c, N);\n",
    "\n",
    "\tfor(int i = 0; i<5;i++){\n",
    "\t\tprintf(\"%f \", h_c[i]);\n",
    "\t}\n",
    "\n",
    "\tprintf(\"\\n\");\n",
    "\n",
    "\tfor(int i = N-1; i>N-6;i--){\n",
    "\t\tprintf(\"%f \", h_c[i]);\n",
    "\t}\n",
    "\n",
    "\tprintf(\"\\n\");\n",
    "\n",
    "\tfree(h_a);\n",
    "\tfree(h_b);\n",
    "\tfree(h_c);\n",
    "    \n",
    "    end = clock();\n",
    "    double dif = (double)(end - start)*1000.0 / CLOCKS_PER_SEC;\n",
    "    printf(\"time : %f ms\\n\", dif);\n",
    "\treturn 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2093,
     "status": "ok",
     "timestamp": 1551031869634,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "4mnkmrMARjlL",
    "outputId": "84f33ecc-e8fd-455b-9745-665b04b24e46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01m\u001b[Kvecadd.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kvecadd.c:18:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit declaration of function ‘\u001b[01m\u001b[Kmalloc\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wimplicit-function-declaration\u001b[m\u001b[K]\n",
      "  float* h_a = (float*) \u001b[01;35m\u001b[Kmalloc\u001b[m\u001b[K(size);\n",
      "                        \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kvecadd.c:18:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kincompatible implicit declaration of built-in function ‘\u001b[01m\u001b[Kmalloc\u001b[m\u001b[K’\n",
      "\u001b[01m\u001b[Kvecadd.c:18:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kinclude ‘\u001b[01m\u001b[K<stdlib.h>\u001b[m\u001b[K’ or provide a declaration of ‘\u001b[01m\u001b[Kmalloc\u001b[m\u001b[K’\n",
      "\u001b[01m\u001b[Kvecadd.c:43:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit declaration of function ‘\u001b[01m\u001b[Kfree\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wimplicit-function-declaration\u001b[m\u001b[K]\n",
      "  \u001b[01;35m\u001b[Kfree\u001b[m\u001b[K(h_a);\n",
      "  \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kvecadd.c:43:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kincompatible implicit declaration of built-in function ‘\u001b[01m\u001b[Kfree\u001b[m\u001b[K’\n",
      "\u001b[01m\u001b[Kvecadd.c:43:2:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kinclude ‘\u001b[01m\u001b[K<stdlib.h>\u001b[m\u001b[K’ or provide a declaration of ‘\u001b[01m\u001b[Kfree\u001b[m\u001b[K’\n",
      "4.200000 4.200000 4.200000 4.200000 4.200000 \n",
      "4.200000 4.200000 4.200000 4.200000 4.200000 \n",
      "time : 403.330000 ms\n"
     ]
    }
   ],
   "source": [
    "!(gcc -o vecadd2 vecadd.c && ./vecadd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_pvG9n9ORjlM"
   },
   "source": [
    "## Can we go faster?\n",
    "\n",
    "Yes, we can.\n",
    "Cuda provides the way to make our program more parallel then it is, introducing `streams`.\n",
    "\n",
    "Cuda streams is pretty strate forward to create and use. In below code we create array of type `cudaStream_t`, and initialize in using `cudaStreamCreate()`, then we use `cudaMemcpyAsync` copy memory asynchronously from host and other streams.\n",
    "\n",
    "**Note:** that VecAdd call will implicitly synchronize the stream, so it will be called only after the `cudaMemcpyAsync` is finished. Also don't forget to clean up using `cudaStreamDestroy`, destruction will be performed only after every thread on this stream is finished (so we synchronize host with this stream). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1USyv79RjlN"
   },
   "source": [
    "## Memory management \n",
    "\n",
    "Another way to boost your program is to use different memory type f.e.g. `cudaMallocHost` allocates page-locked memory, that can speed up aplications that often accessing memory.\n",
    "\n",
    "(You can compare it by uncommenting blocks of code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1551031910606,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "JtPfVYX5RjlN",
    "outputId": "b26808fc-fa7a-4038-826b-3f70a7c325b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streams.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile streams.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <time.h> \n",
    "\n",
    "__global__ void VecAdd(float* A, float* B, float* C, int N){\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(i < N){\n",
    "        C[i] = A[i] + B[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void) {\n",
    "    int N = 9000000;\n",
    "    size_t size = N*sizeof(float);\n",
    "    \n",
    "    /*float* h_a = (float*)malloc(2*size);\n",
    "    float* h_b = (float*)malloc(2*size);\n",
    "    float* h_c = (float*)malloc(2*size);*/\n",
    "    \n",
    "    \n",
    "    float* h_a;\n",
    "    float* h_b;\n",
    "    float* h_c;\n",
    "    cudaMallocHost(&h_a, 2*size);\n",
    "    cudaMallocHost(&h_b, 2*size);\n",
    "    cudaMallocHost(&h_c, 2*size);\n",
    "    \n",
    "    for(int i = 0;i<2*N;i++){\n",
    "        h_a[i] = 1.5;\n",
    "        h_b[i] = 2.7;\n",
    "    }\n",
    "    \n",
    "    float* d_a;\n",
    "    cudaMalloc(&d_a, 2*size);\n",
    "\n",
    "    float* d_b;\n",
    "    cudaMalloc(&d_b, 2*size);\n",
    "\n",
    "    float* d_c;\n",
    "    cudaMalloc(&d_c, 2*size);\n",
    "    \n",
    "    cudaStream_t stream[2];\n",
    "    for(int i = 0; i < 2;i++){\n",
    "        cudaStreamCreate(&stream[i]);\n",
    "    }\n",
    "    \n",
    "    int threads_per_block = 256;\n",
    "    int blocks_per_grid = (N + threads_per_block - 1) / threads_per_block;\n",
    "    \n",
    "    for(int i = 0;i<2;i++){\n",
    "        cudaMemcpyAsync(d_a + i * size, h_a + i*size, size, cudaMemcpyHostToDevice, stream[i]);\n",
    "        cudaMemcpyAsync(d_b + i * size, h_b + i*size, size, cudaMemcpyHostToDevice, stream[i]);\n",
    "        \n",
    "        VecAdd<<<blocks_per_grid, threads_per_block, 0, stream[i]>>>(d_a + i*size, d_b + i*size, d_c + i*size, N);\n",
    "        \n",
    "        cudaMemcpyAsync(h_c + i*size, d_c + i*size, size, cudaMemcpyDeviceToHost, stream[i]);\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    for(int i = 0;i<2;i++){\n",
    "        cudaStreamDestroy(stream[i]);\n",
    "    }\n",
    "\n",
    "    for(int i = 0; i<5;i++){\n",
    "        printf(\"%f \", h_c[i]);\n",
    "    }\n",
    "\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    for(int i = N-1; i>N-6;i--){\n",
    "        printf(\"%f \", h_c[i]);\n",
    "    }\n",
    "\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "\n",
    "    /*free(h_a);\n",
    "    free(h_b);\n",
    "    free(h_c);*/\n",
    "    \n",
    "    cudaFreeHost(h_a);\n",
    "    cudaFreeHost(h_b);\n",
    "    cudaFreeHost(h_c);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2963,
     "status": "ok",
     "timestamp": 1551031913879,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "CGUfvh7GRjlP",
    "outputId": "c077d007-a5f9-49bf-8fcb-a2ddb96907f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.200000 4.200000 4.200000 4.200000 4.200000 \n",
      "4.200000 4.200000 4.200000 4.200000 4.200000 \n"
     ]
    }
   ],
   "source": [
    "!nvcc -o stream streams.cu --run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2969,
     "status": "ok",
     "timestamp": 1551031915851,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "MdJj8yTkRjlR",
    "outputId": "9f735176-a152-4b03-a10e-662f57c69670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==1023== NVPROF is profiling process 1023, command: ./stream\n",
      "4.200000 4.200000 4.200000 4.200000 4.200000 \n",
      "4.200000 4.200000 4.200000 4.200000 4.200000 \n",
      "==1023== Profiling application: ./stream\n",
      "==1023== Warning: 5 records have invalid timestamps due to insufficient device buffer space. You can configure the buffer space using the option --device-buffer-size.\n",
      "==1023== Warning: 3 records have invalid timestamps due to insufficient semaphore pool size. You can configure the pool size using the option --profiling-semaphore-pool-size.\n",
      "==1023== Profiling result:\n",
      "No kernels were profiled.\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "      API calls:   79.43%  231.53ms         3  77.177ms  31.128ms  167.97ms  cudaHostAlloc\n",
      "                   16.83%  49.057ms         1  49.057ms  49.057ms  49.057ms  cudaDeviceSynchronize\n",
      "                    3.23%  9.4168ms         2  4.7084ms  13.622us  9.4032ms  cudaLaunchKernel\n",
      "                    0.28%  807.00us         3  269.00us  205.56us  369.41us  cudaMalloc\n",
      "                    0.12%  345.30us        96  3.5960us     151ns  194.51us  cuDeviceGetAttribute\n",
      "                    0.07%  190.75us         1  190.75us  190.75us  190.75us  cuDeviceTotalMem\n",
      "                    0.02%  50.158us         6  8.3590us     805ns  25.350us  cudaMemcpyAsync\n",
      "                    0.02%  46.420us         2  23.210us  9.7430us  36.677us  cudaStreamCreate\n",
      "                    0.01%  23.018us         1  23.018us  23.018us  23.018us  cuDeviceGetName\n",
      "                    0.00%  13.595us         3  4.5310us     760ns  11.902us  cudaFree\n",
      "                    0.00%  4.9410us         2  2.4700us     783ns  4.1580us  cudaStreamDestroy\n",
      "                    0.00%  3.3090us         3  1.1030us     719ns  1.8300us  cudaFreeHost\n",
      "                    0.00%  3.0610us         1  3.0610us  3.0610us  3.0610us  cuDeviceGetPCIBusId\n",
      "                    0.00%  2.2270us         3     742ns     189ns  1.1310us  cuDeviceGetCount\n",
      "                    0.00%  1.2760us         2     638ns     390ns     886ns  cuDeviceGet\n",
      "                    0.00%     329ns         1     329ns     329ns     329ns  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKg53M1kRjlT"
   },
   "source": [
    "## Matrix multiplication speed up\n",
    "\n",
    "Now we can write matrix multiplication example and compare in to CPU-only version.\n",
    "\n",
    "In this example we use dim3 type to create two-dimensional blocks, in two-dimensional grid, each thread is responsible for calculating 1 result element. (see image below) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fd1ZpVXXRjlT"
   },
   "source": [
    "![mat-mul](https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-without-shared-memory.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1551031939475,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "wVyS-AHmRjlU",
    "outputId": "19a8e9ba-3bf4-42c0-80af-a3570a7fadff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting matrixMult.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matrixMult.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "typedef struct {\n",
    "    int width;\n",
    "    int height;\n",
    "    float* elements;\n",
    "} Matrix;\n",
    "\n",
    "#define BLOCK_SIZE 16\n",
    "\n",
    "__global__ void MatMul(Matrix A, Matrix B, Matrix C){\n",
    "    float val = 0;\n",
    "    int row = blockDim.x*blockIdx.x + threadIdx.x;\n",
    "    int col = blockDim.y*blockIdx.y + threadIdx.y;\n",
    "    \n",
    "    if(row < A.height && col < A.width){\n",
    "        for(int i = 0;i < A.width; i++){\n",
    "            val += A.elements[row*A.width + i] * B.elements[i*B.width + col];\n",
    "        }\n",
    "    \n",
    "        C.elements[row*C.width + col] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void){\n",
    "    Matrix h_A;\n",
    "    Matrix h_B;\n",
    "    Matrix h_C;\n",
    "    h_A.width = h_A.height = 3000;\n",
    "    h_B.width = h_B.height = 3000;\n",
    "    h_C.width = h_C.height = 3000;\n",
    "    \n",
    "    size_t size = h_A.height*h_A.width*sizeof(float);\n",
    "    \n",
    "    h_A.elements = (float*)malloc(size);\n",
    "    h_B.elements = (float*)malloc(size);\n",
    "    h_C.elements = (float*)malloc(size);\n",
    "    \n",
    "    for(int i = 0;i<h_A.height;i++){\n",
    "        for(int j = 0;j<h_A.width;j++){\n",
    "            h_A.elements[i*h_A.width + j] = 1;\n",
    "            h_B.elements[i*h_B.width + j] = 2;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    Matrix d_A;\n",
    "    d_A.width = h_A.width; d_A.height = h_A.height;\n",
    "    cudaMalloc(&d_A.elements, size);\n",
    "    cudaMemcpy(d_A.elements, h_A.elements, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    Matrix d_B;\n",
    "    d_B.width = h_B.width; d_B.height = h_B.height;\n",
    "    cudaMalloc(&d_B.elements, size);\n",
    "    cudaMemcpy(d_B.elements, h_B.elements, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    Matrix d_C;\n",
    "    d_C.width = h_C.width; d_C.height = h_C.height;\n",
    "    cudaMalloc(&d_C.elements, size);\n",
    "    \n",
    "    dim3 threads_per_block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    //dim3 block_per_grid((d_A.height + BLOCK_SIZE -1) / BLOCK_SIZE, (d_A.width + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    dim3 block_per_grid((d_A.height) / BLOCK_SIZE, (d_A.width) / BLOCK_SIZE);\n",
    "    \n",
    "    MatMul<<<block_per_grid, threads_per_block>>>(d_A, d_B, d_C);\n",
    "    \n",
    "    cudaMemcpy(h_C.elements, d_C.elements, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"%f\\n\", h_C.elements[0]);\n",
    "    \n",
    "    cudaFree(d_A.elements);\n",
    "    cudaFree(d_B.elements);\n",
    "    cudaFree(d_C.elements);\n",
    "    \n",
    "    free(h_A.elements);\n",
    "    free(h_B.elements);\n",
    "    free(h_C.elements);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6106,
     "status": "ok",
     "timestamp": 1551031946481,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "xPvo_kOSRjlV",
    "outputId": "1f348af6-4dfd-4913-ac51-d1467386db42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000.000000\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o matrixMult matrixMult.cu --run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6212,
     "status": "ok",
     "timestamp": 1551031954696,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "84TeQNAVRjlW",
    "outputId": "0112bf8f-1be9-42eb-8ff0-b020909b167e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==1078== NVPROF is profiling process 1078, command: ./matrixMult\n",
      "6000.000000\n",
      "==1078== Profiling application: ./matrixMult\n",
      "==1078== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   99.20%  4.10240s         1  4.10240s  4.10240s  4.10240s  MatMul(Matrix, Matrix, Matrix)\n",
      "                    0.55%  22.728ms         1  22.728ms  22.728ms  22.728ms  [CUDA memcpy DtoH]\n",
      "                    0.25%  10.542ms         2  5.2710ms  5.2678ms  5.2741ms  [CUDA memcpy HtoD]\n",
      "      API calls:   96.37%  4.13661s         3  1.37887s  5.3041ms  4.12599s  cudaMemcpy\n",
      "                    3.39%  145.48ms         3  48.493ms  205.45us  145.02ms  cudaMalloc\n",
      "                    0.22%  9.6082ms         3  3.2027ms  231.87us  4.6934ms  cudaFree\n",
      "                    0.01%  329.65us        96  3.4330us     152ns  152.22us  cuDeviceGetAttribute\n",
      "                    0.01%  266.78us         1  266.78us  266.78us  266.78us  cuDeviceTotalMem\n",
      "                    0.00%  115.03us         1  115.03us  115.03us  115.03us  cudaLaunchKernel\n",
      "                    0.00%  20.710us         1  20.710us  20.710us  20.710us  cuDeviceGetName\n",
      "                    0.00%  3.0100us         1  3.0100us  3.0100us  3.0100us  cuDeviceGetPCIBusId\n",
      "                    0.00%  1.9690us         3     656ns     155ns  1.1670us  cuDeviceGetCount\n",
      "                    0.00%  1.2650us         2     632ns     255ns  1.0100us  cuDeviceGet\n",
      "                    0.00%     322ns         1     322ns     322ns     322ns  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./matrixMult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1551030526248,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "3ytABja1RjlX",
    "outputId": "98b5f6ec-2d26-43c5-e26f-53bb71c50bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matrixMult_cpu.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile matrixMult_cpu.c\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <time.h>\n",
    "\n",
    "typedef struct {\n",
    "    int width;\n",
    "    int height;\n",
    "    float* elements;\n",
    "} Matrix;\n",
    "\n",
    "int main(void){\n",
    "    Matrix h_A;\n",
    "    Matrix h_B;\n",
    "    Matrix h_C;\n",
    "    h_A.width = h_A.height = 3000;\n",
    "    h_B.width = h_B.height = 3000;\n",
    "    h_C.width = h_C.height = 3000;\n",
    "    \n",
    "    size_t size = h_A.height*h_A.width*sizeof(float);\n",
    "    \n",
    "    h_A.elements = (float*)malloc(size);\n",
    "    h_B.elements = (float*)malloc(size);\n",
    "    h_C.elements = (float*)malloc(size);\n",
    "    \n",
    "    for(int i = 0;i<h_A.height;i++){\n",
    "        for(int j = 0;j<h_A.width;j++){\n",
    "            h_A.elements[i*h_A.width + j] = 1;\n",
    "            h_B.elements[i*h_B.width + j] = 2;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    clock_t start,end;\n",
    "    \n",
    "    start = clock();\n",
    "    \n",
    "    for(int i = 0;i<h_A.height;i++){\n",
    "        for(int j = 0;j<h_A.width;j++){\n",
    "            float val = 0;\n",
    "            for(int k = 0;k<h_A.width;k++){\n",
    "                val += h_A.elements[i*h_A.width + k] * h_B.elements[k*h_B.width + j];\n",
    "            }\n",
    "            h_C.elements[i*h_C.width + j] = val;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    end = clock();\n",
    "    double dif = (double)(end - start)*1000.0 / CLOCKS_PER_SEC;\n",
    "    printf(\"time : %f ms\\n\", dif);\n",
    "    \n",
    "    printf(\"%f\\n\", h_C.elements[0]);\n",
    "    \n",
    "    free(h_A.elements);\n",
    "    free(h_B.elements);\n",
    "    free(h_C.elements);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "viADNg8FRjlZ",
    "outputId": "80e1de93-fc3b-4607-9b27-85f098d9a662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01m\u001b[KmatrixMult_cpu.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[KmatrixMult_cpu.c:21:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit declaration of function ‘\u001b[01m\u001b[Kmalloc\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wimplicit-function-declaration\u001b[m\u001b[K]\n",
      "     h_A.elements = (float*)\u001b[01;35m\u001b[Kmalloc\u001b[m\u001b[K(size);\n",
      "                            \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[KmatrixMult_cpu.c:21:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kincompatible implicit declaration of built-in function ‘\u001b[01m\u001b[Kmalloc\u001b[m\u001b[K’\n",
      "\u001b[01m\u001b[KmatrixMult_cpu.c:21:28:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kinclude ‘\u001b[01m\u001b[K<stdlib.h>\u001b[m\u001b[K’ or provide a declaration of ‘\u001b[01m\u001b[Kmalloc\u001b[m\u001b[K’\n",
      "\u001b[01m\u001b[KmatrixMult_cpu.c:52:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit declaration of function ‘\u001b[01m\u001b[Kfree\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wimplicit-function-declaration\u001b[m\u001b[K]\n",
      "     \u001b[01;35m\u001b[Kfree\u001b[m\u001b[K(h_A.elements);\n",
      "     \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[KmatrixMult_cpu.c:52:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kincompatible implicit declaration of built-in function ‘\u001b[01m\u001b[Kfree\u001b[m\u001b[K’\n",
      "\u001b[01m\u001b[KmatrixMult_cpu.c:52:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kinclude ‘\u001b[01m\u001b[K<stdlib.h>\u001b[m\u001b[K’ or provide a declaration of ‘\u001b[01m\u001b[Kfree\u001b[m\u001b[K’\n"
     ]
    }
   ],
   "source": [
    "!gcc -o matrixMult_cpu matrixMult_cpu.c && ./matrixMult_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jIIjYf-Rjla"
   },
   "source": [
    "## Matrix multiplication speed up V. 2\n",
    "\n",
    "We can notice that while calculating result, we use each matrix element multiple times. \n",
    "**Can we use this observation to speed up our program?**\n",
    "\n",
    "It's turns out that yes, but firstly, we should introduce some `device memory hierarchy`.\n",
    "\n",
    "![hierarchy](https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/memory-hierarchy.png)\n",
    "\n",
    "As we can see each thread have it's own local memory, and each block have it's own memory(shared for every thread in the block), as well as global memory accessible from every thread in every grid\n",
    "\n",
    "## Shared memory\n",
    "\n",
    "It's turns out that this 'block local' memory is much fuster than global memory. We can compare it to L3 cache of processor. \n",
    "\n",
    "To define someting as shared we use `__shared__` keyword. (see code example)\n",
    "\n",
    "\n",
    "## Code example explanation\n",
    "\n",
    "So in this code we try to take advantage of shared memory speed. Lets develop some intuition on how it works:\n",
    "Each block is responcible for computing Csub matrix of size BLOCK_SIZE x BLOCK_SIZE. \n",
    "\n",
    "Taking into account that shared memory is comperably small, we use for loop to divide A and B to (A.width / BLOCK_SIZE) and (B.heigth / BLOCK_SIZE) respectively sum-matrices that we use to compute Csub.(see image below)\n",
    "\n",
    "**Note:** `__device__` keyword is used to define the divice(GPU) function that can only be called from device.\n",
    "          `__syncthreads()` is one of the methods to explicitly synchronize threads in block, so every thread waits             other threads to reach this point before continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5E208uTYRjlb"
   },
   "source": [
    "![mat-mul-shared](https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-with-shared-memory.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1551031963539,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "TxUlzzI2Rjlb",
    "outputId": "0df7f139-286c-49ff-e8ef-e72d94b2462a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting matrixMult_shared.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matrixMult_shared.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "typedef struct {\n",
    "    int stride;\n",
    "    int width;\n",
    "    int height;\n",
    "    float* elements;\n",
    "} Matrix;\n",
    "\n",
    "#define BLOCK_SIZE 16\n",
    "\n",
    "__device__ Matrix GetSubMatrix(Matrix A, int row, int col){\n",
    "    Matrix Asub;\n",
    "    Asub.width = Asub.height = BLOCK_SIZE;\n",
    "    Asub.stride = A.stride;\n",
    "    Asub.elements = &A.elements[row*A.stride*BLOCK_SIZE + col*BLOCK_SIZE];\n",
    "    \n",
    "    return Asub;\n",
    "}\n",
    "\n",
    "__device__ float GetElement(Matrix A, int row, int col){\n",
    "    return A.elements[row*A.stride + col];\n",
    "}\n",
    "\n",
    "__device__ void SetElement(Matrix A, int row, int col, int val){\n",
    "    A.elements[row*A.stride + col] = val;\n",
    "}\n",
    "\n",
    "__global__ void MatMul(Matrix A, Matrix B, Matrix C){\n",
    "    int blockRow = blockIdx.x;\n",
    "    int blockCol = blockIdx.y;\n",
    "    \n",
    "    Matrix Csub = GetSubMatrix(C, blockRow, blockCol);\n",
    "    \n",
    "    float val = 0;\n",
    "    \n",
    "    int row = threadIdx.x;\n",
    "    int col = threadIdx.y;\n",
    "    \n",
    "    for(int i = 0;i<(A.width / BLOCK_SIZE);i++){\n",
    "        Matrix Asub = GetSubMatrix(A, blockRow, i);\n",
    "        Matrix Bsub = GetSubMatrix(B, i, blockCol);\n",
    "        \n",
    "        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n",
    "        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n",
    "        \n",
    "        As[row][col] = GetElement(Asub, row, col);\n",
    "        Bs[row][col] = GetElement(Bsub, row, col);\n",
    "        __syncthreads();\n",
    "        \n",
    "        for(int j = 0;j<BLOCK_SIZE;j++){\n",
    "            val += As[row][j]*Bs[j][col];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    SetElement(Csub, row, col, val);\n",
    "}\n",
    "\n",
    "int main(void){\n",
    "    Matrix h_A;\n",
    "    Matrix h_B;\n",
    "    Matrix h_C;\n",
    "    h_A.stride = h_A.width = h_A.height = 4096;\n",
    "    h_B.stride = h_B.width = h_B.height = 4096;\n",
    "    h_C.stride = h_C.width = h_C.height = 4096;\n",
    "    \n",
    "    size_t size = h_A.height*h_A.width*sizeof(float);\n",
    "    \n",
    "    h_A.elements = (float*)malloc(size);\n",
    "    h_B.elements = (float*)malloc(size);\n",
    "    h_C.elements = (float*)malloc(size);\n",
    "    \n",
    "    for(int i = 0;i<h_A.height;i++){\n",
    "        for(int j = 0;j<h_A.width;j++){\n",
    "            h_A.elements[i*h_A.width + j] = 1;\n",
    "            h_B.elements[i*h_B.width + j] = 2;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    Matrix d_A;\n",
    "    d_A.stride = d_A.width = h_A.width; d_A.height = h_A.height;\n",
    "    cudaMalloc(&d_A.elements, size);\n",
    "    cudaMemcpy(d_A.elements, h_A.elements, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    Matrix d_B;\n",
    "    d_B.stride = d_B.width = h_B.width; d_B.height = h_B.height;\n",
    "    cudaMalloc(&d_B.elements, size);\n",
    "    cudaMemcpy(d_B.elements, h_B.elements, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    Matrix d_C;\n",
    "    d_C.stride = d_C.width = h_C.width; d_C.height = h_C.height;\n",
    "    cudaMalloc(&d_C.elements, size);\n",
    "    \n",
    "    dim3 threads_per_block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 block_per_grid((d_A.height) / BLOCK_SIZE, (d_A.width) / BLOCK_SIZE);\n",
    "    \n",
    "    MatMul<<<block_per_grid, threads_per_block>>>(d_A, d_B, d_C);\n",
    "    \n",
    "    cudaMemcpy(h_C.elements, d_C.elements, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    int check = 1;\n",
    "    \n",
    "    for(int i = 0;i<h_C.height;i++){\n",
    "        for(int j = 0;j<h_C.width;j++){\n",
    "            if(h_C.elements[i*h_C.width + j] != 8192){\n",
    "                check=0;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"check result: %d\\n\", check);\n",
    "    \n",
    "    cudaFree(d_A.elements);\n",
    "    cudaFree(d_B.elements);\n",
    "    cudaFree(d_C.elements);\n",
    "    \n",
    "    free(h_A.elements);\n",
    "    free(h_B.elements);\n",
    "    free(h_C.elements);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4715,
     "status": "ok",
     "timestamp": 1551031969399,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "V_zJngyrRjlc",
    "outputId": "26743e15-1555-44ce-d417-a5172628e15d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check result: 1\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o matrixMult_shared matrixMult_shared.cu --run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4622,
     "status": "ok",
     "timestamp": 1551031975546,
     "user": {
      "displayName": "Dima Zhylko",
      "photoUrl": "",
      "userId": "07621898023958018876"
     },
     "user_tz": -60
    },
    "id": "SCLMOrzhRjld",
    "outputId": "5d42fd39-801b-49bf-a2a4-0edf43305ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==1130== NVPROF is profiling process 1130, command: ./matrixMult_shared\n",
      "check result: 1\n",
      "==1130== Profiling application: ./matrixMult_shared\n",
      "==1130== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   97.45%  2.42612s         1  2.42612s  2.42612s  2.42612s  MatMul(Matrix, Matrix, Matrix)\n",
      "                    1.76%  43.736ms         1  43.736ms  43.736ms  43.736ms  [CUDA memcpy DtoH]\n",
      "                    0.79%  19.788ms         2  9.8942ms  9.7593ms  10.029ms  [CUDA memcpy HtoD]\n",
      "      API calls:   93.59%  2.49100s         3  830.33ms  9.7794ms  2.47116s  cudaMemcpy\n",
      "                    5.73%  152.57ms         3  50.856ms  265.59us  151.94ms  cudaMalloc\n",
      "                    0.65%  17.363ms         3  5.7875ms  255.79us  8.5669ms  cudaFree\n",
      "                    0.01%  372.29us        96  3.8780us     187ns  166.21us  cuDeviceGetAttribute\n",
      "                    0.01%  181.72us         1  181.72us  181.72us  181.72us  cuDeviceTotalMem\n",
      "                    0.00%  109.45us         1  109.45us  109.45us  109.45us  cudaLaunchKernel\n",
      "                    0.00%  23.500us         1  23.500us  23.500us  23.500us  cuDeviceGetName\n",
      "                    0.00%  3.1210us         1  3.1210us  3.1210us  3.1210us  cuDeviceGetPCIBusId\n",
      "                    0.00%  2.2850us         3     761ns     189ns  1.2750us  cuDeviceGetCount\n",
      "                    0.00%  1.7200us         2     860ns     534ns  1.1860us  cuDeviceGet\n",
      "                    0.00%     287ns         1     287ns     287ns     287ns  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./matrixMult_shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ix6cCQfHRjle"
   },
   "source": [
    "## Yey! \n",
    "\n",
    "I hope that after this, you have high concepts of what's CUDA about\n",
    "\n",
    "## Further reading \n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html - NVIDIA Documentation(also place where I stole images and some code examples) \n",
    "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html \n",
    "\n",
    "https://courses.nvidia.com/courses/course-v1:DLI+A-AC-00+V1/about - free NVIDIA course on CUDA\n",
    "\n",
    "all images are from https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uheoJRSRjle"
   },
   "source": [
    "## Contact me\n",
    "\n",
    "zhylko.dima@gmail.com\n",
    "\n",
    "https://www.facebook.com/profile.php?id=100009572680557"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BIT_AI.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
