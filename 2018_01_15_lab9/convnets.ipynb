{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi!\n",
    "\n",
    "Today we'll get back to Neural Networks and take a look at how image processing is done. We'll talk about current state-of the art approach - Convolutional Neural Networks. We'll also talk about a brand new approach - Capsule Networks - that may or may not dethrone CNNs in the future ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "As a very basic example of images, the MNIST dataset will accompaby us through the rest of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_mnist(i):\n",
    "    pixels = mnist.train.images[i].reshape(28,28)\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    labels = mnist.train.labels[i]\n",
    "    print(labels)\n",
    "    print(np.argmax(labels))\n",
    "    \n",
    "    \n",
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)\n",
    "print(mnist.test.images.shape)\n",
    "print(mnist.test.labels.shape)\n",
    "\n",
    "# notice that the data is already normalized. Yay!\n",
    "print(mnist.train.images.max()) \n",
    "print(mnist.train.images.min())\n",
    "\n",
    "interact(present_mnist,\n",
    "        i=widgets.IntSlider(min=0, max=100)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's classify it!\n",
    "\n",
    "We've already discussed various vays to approach classification of this dataset. \n",
    "We'll rephrase some of them below. You can find further explanations in **lab4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's always good to start with linear classifier\n",
    "\n",
    "Which, as you may remember is essentially a matrix multiplication!\n",
    "But, for the educational value of this example not to be lost on us, let's now implement it in Tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to deal with low-level variables in tf, everything happens inside a Session\n",
    "outside_W = None\n",
    "with tf.Session() as sess:\n",
    "    # in tensorflow you build computational graphs from variables and operations on them\n",
    "    # and then you can evaluate their outputs based on what you input\n",
    "    # you can think of placeholders as such inputs\n",
    "    \n",
    "    # X = features vector (28 x 28 = 784) - we'll deal with bias differently this time\n",
    "    X = tf.placeholder(tf.float32, shape=(None, 784)) \n",
    "    # L = labels vector - shape 10 because we've got 10 possible classes\n",
    "    L = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "    # W - weights matrix\n",
    "    W = tf.Variable(tf.random_uniform((784,10)))\n",
    "    # b - bias weights - we can simply add them to the output of X @ W multiplication\n",
    "    b = tf.Variable(tf.random_uniform((1,10)))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Y - the outputs of our model - defined as a computation!\n",
    "    Y = X @ W + b\n",
    "    # this is a built-in method to calculate loss \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=L, logits=Y))\n",
    "    \n",
    "    # this is built-in Gradient Descent\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(Y,axis=1), tf.argmax(L,axis=1))\n",
    "    \n",
    "    # this dictionary will tell tensorflow what values to pass to which placeholder (input)\n",
    "    # when we evaluate a computational graph\n",
    "    # here, we pass test images and their respective labels to X and L respectively\n",
    "    eval_dict = {X: mnist.test.images, L: mnist.test.labels}\n",
    "    loss_history = []\n",
    "    \n",
    "    num_iterations = 1000\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # for the training to go quicker, we'll use stochastic gradient descent\n",
    "        # in each iteration we train the model on a random batch of data\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        # analogous to eval_dict\n",
    "        train_dict = {X: batch[0], L: batch[1]}\n",
    "        # one full training step\n",
    "        train_step.run(feed_dict=train_dict)\n",
    "        # evaluating the loss after an iteration\n",
    "        loss_tmp = loss.eval(feed_dict=eval_dict)\n",
    "        loss_history.append(loss_tmp)\n",
    "        \n",
    "    # evaluating the accuracy after training\n",
    "    accuracy_test = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('final accuracy on test data:', accuracy_test.eval(feed_dict=eval_dict))\n",
    "    print('loss graph:')\n",
    "    plt.plot(list(range(num_iterations)), loss_history)\n",
    "    outside_W = sess.run(W) # a litle something that will let us keep the final value of W \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should achieve about 88% accuracy. Not bad! \n",
    "\n",
    "(actually, for MNIST it *is* bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of a (very simple) computational graph\n",
    "<img src=\"img/comp_graph.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digression: Let's visualize what actually excites the classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_W(i, outside_W):\n",
    "    plt.imshow(outside_W[:, i].reshape(28,28), cmap='gray')\n",
    "\n",
    "interact(display_W,\n",
    "         i=widgets.IntSlider(min=0, max=9),\n",
    "         outside_W=widgets.fixed(outside_W)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe a Neural Network will do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we will try out a fully connected (dense) network\n",
    "\n",
    "As you may remember, fully connected network is essentially a bunch of linear classifiers stacked between each other with non-linearities between them:\n",
    "\n",
    "<img src=\"img/neural_dense.jpg\" style=\"width: 500px;\"/>\n",
    "\n",
    "#### Enter Tensorflow's layers and estimators!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIrst, let's write down a function which defines a model to Tensorflow's estimator API. In the function we define model's behaviour in various cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_model_fn(features, labels, mode):\n",
    "    # labels are numbers of classes - let's not mess with API standards :D\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "\n",
    "    input_layer = features[\"x\"]\n",
    "    # hidden layers\n",
    "    dense_1 = tf.layers.dense(inputs=input_layer, units=200, activation=tf.nn.relu)\n",
    "    dense_2 = tf.layers.dense(inputs=dense_1, units=200, activation=tf.nn.relu)\n",
    "    # output layer\n",
    "    logits = tf.layers.dense(inputs=dense_2, units=10, activation=tf.nn.relu)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # We can use different optimizers!\n",
    "#         optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        \n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step() #keeps track of steps taken\n",
    "        )\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have the function, we can create an instance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dense_classifier = tf.estimator.Estimator(model_fn=dense_model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model, we need to specify the training input. It's also a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": mnist.train.images},\n",
    "    y=mnist.train.labels.argmax(axis=1),\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can perform the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dense_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_dense(layers, id):\n",
    "#     plt.imshow(lyers)\n",
    "\n",
    "interact(display_W,\n",
    "         i=widgets.IntSlider(min=0, max=199),\n",
    "         outside_W=widgets.fixed(mnist_dense_classifier.get_variable_value('dense/kernel'))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to evaluate the performance of the model, we first define the testing input function (just like above). Then, on evaluation we get the metrics we specified in the model_fn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": mnist.test.images},\n",
    "    y=mnist.test.labels.argmax(axis=1),\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "eval_results = mnist_dense_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woooow! 97% with just two hidden layers!\n",
    "\n",
    "Feel free to experiment with different numbers of layers and their sizes ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter Convolutional Neural Networks!\n",
    "\n",
    "<img src=img/conv_net.png/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "        filter=None,\n",
    "      filters=32,\n",
    "      strides=(1,1), # how many pixels it skips before making the next record\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        inputs=conv1, \n",
    "        pool_size=[2, 2], \n",
    "        strides=2 #means the same thing as (2,2) -> how many neighboring outputs of conv layer are taken into account\n",
    "    )\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    #   onehot_labels = labels\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_cnn_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = mnist_cnn_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter capsule nets!\n",
    "\n",
    "There is already an excellent notebook by Aurelien Geron explaining the whole idea here - we'll proceed with it now! ;)\n",
    "\n",
    "https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb\n",
    "\n",
    "I tweaked it a bit, so now please swich to the one in this directory!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
