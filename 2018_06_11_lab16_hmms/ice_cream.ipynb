{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/hmmlearn/hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observable and Hidden States\n",
    "Hidden Markov Models (HMMs) are useful when analyzing time series. \n",
    "\n",
    "To define a HMM, we must define the following:\n",
    "\n",
    "* **observable states** - measurable data which serves as an input to HMM. You could think of it as something analogous to features of datapoints.\n",
    "* **hidden states** - information we want to infer from a series of observations. The idea is that hidden states an observations are somehow correlated.\n",
    "\n",
    "Moreover, the assumption is that hidden states satisfy the **Markov property** - the value of next state in the series depends only on the value of the previous state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What constitutes a HMM?\n",
    "As for the HMMs themselves, a trained model posesses the following information:\n",
    "\n",
    "* probability distribution of the initial hidden state in the series -$I$\n",
    "* probabilities of transitions between hidden states - $T$ \n",
    "* emission probablities - given a particular hidden state, how likely are we to make a particular observation? - $E$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice cream!\n",
    "\n",
    "In this notebook we'll play with a classic ice cream example (it's July after all, y'all!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hidden states - is the day cold or hot?\n",
    "hidden_states = {'cold': 0,\n",
    "                 'hot': 1\n",
    "                }\n",
    "# observable states - how many ice creams have been eaten on that day?\n",
    "observable_states = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMMs have three primary use cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood - the forward algorithm\n",
    "\n",
    "In this case, we're working with a pre trained HMM - which means all the essential parameters must be set manually.\n",
    "Having this pretrained model, we can calculate the probability of given sequence of events occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = hmm.MultinomialHMM(n_components = len(hidden_states), verbose=True)\n",
    "\n",
    "# initial probability vector of length n_hidden_states\n",
    "# intuition - how probable is it that the first day is cold/hot?\n",
    "model.startprob_ = np.array([0.5, 0.5])\n",
    "\n",
    "# transition matrix - likelihood of transitioning between hidden states\n",
    "# shape = n_hidden_states x n_hidden_states\n",
    "# intuition - given that the current day is cold/hot, how likely is it that the next day will be cold/hot?\n",
    "model.transmat_ = np.array([\n",
    "  [0.7, 0.3],\n",
    "  [0.4, 0.6]\n",
    "])\n",
    "\n",
    "# emission matrix - likelihood of particular observation given each hidden state\n",
    "# shape = n_hidden_states x n_observable_states\n",
    "# intuition - given that the current day is cold/hot how likely are we to eat 0/1/2 ice creams?\n",
    "model.emissionprob_ = np.array([\n",
    "  [0.7, 0.3, 0.0],\n",
    "  [0.0, 0.4, 0.6]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability is calculated using the *forward algorithm*. It's main chunks are:\n",
    "\n",
    "We know the probability distribution of hidden states in the initial timestep. For every next timestep:\n",
    "\n",
    "If in the $n-1^{th}$ timestep the probabilities of particular hidden states are: \n",
    "\n",
    "$$\n",
    "H^1_{n-1} \\\\\n",
    "H^2_{n-1}\\\\\n",
    "H^3_{n-1}\n",
    "$$\n",
    "\n",
    "Then the likelihoods of hidden states in the $n^{th}$ timestep are:\n",
    "\n",
    "$$\n",
    "H^1_n = H^1_{n-1} \\cdot T_{1 | 1} + H^2_{n-1} \\cdot T_{1 | 2} \\cdot H^3_{n-1} \\cdot T_{1 | 3}\n",
    "$$\n",
    "\n",
    "...and analogusly for $H^2_n$ and $H^3_n$.\n",
    "\n",
    "Then, we can calculiate the likelihood of making a particular observation in the $n^{th}$ timestep:\n",
    "\n",
    "$$\n",
    "O^1_n =  H^1_{n} \\cdot E_{1 | 1} + H^2_{n} \\cdot T_{1 | 2} \\cdot H^3_{n} \\cdot T_{1 | 3} \n",
    "$$\n",
    "\n",
    "... and analogously for $O^2_n$.\n",
    "\n",
    "Finally, having calculated likelihood of given observations in the given timesteps, we can multiply the likelihoods of the observations we've actually made and get our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.atleast_2d([0, 0, 2]).T\n",
    "np.e ** model.score(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most likely sequence - Viterbi algorithm\n",
    "\n",
    "Given a sequence of observations, we want to find the most likely sequence of hidden states that fits the observations. This is called a *Viterbi Path* and is obtained using recursion:\n",
    "\n",
    "The probability that a time series of length $t$ which ends with a hidden state $s_t$ and observation $o_t$ is obtained with:\n",
    "\n",
    "$$\n",
    "V^{s_t}_t = max_{h \\in H} (E_{o_t | s_t} \\cdot T_{s_t | h} \\cdot V^h_{t-1} ) \\\\\n",
    "V^{s_1}_1 = E_{o_1 | s_1} \\cdot I_{s_1}\n",
    "$$\n",
    "\n",
    "\n",
    "The task is to find for the final observation ($o_t$) a hidden state ($s_t$) for which $V^{s_t}_t$ will be maximal. For that, we have to recursively find that for $t-1$, $t-2$, etc...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.atleast_2d([0, 0, 2]).T\n",
    "\n",
    "logprob, states = model.decode(observations)\n",
    "[list(hidden_states.keys())[s] for s in states]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a HMM\n",
    "\n",
    "This is the most difficult problem and there is no analytical way to solve it. There are some iterative algorithms, for example gradient-based methods. Some approaches are discussed here:\n",
    "\n",
    "https://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll focus on an example in the code:\n",
    "\n",
    "\n",
    "First some helper functions for data generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_distribution(center, distr_range, n_categories):\n",
    "    assert(center in range(n_categories))\n",
    "    assert(distr_range < n_categories)\n",
    "    broad_result = np.zeros(2*n_categories)\n",
    "    distr_range += 1\n",
    "    for i in range(distr_range):\n",
    "        broad_result[n_categories + i] = broad_result[n_categories - i] = distr_range - i\n",
    "        \n",
    "    left = n_categories - center\n",
    "    right = 2*n_categories - center\n",
    "    result = broad_result[left:right]\n",
    "    return result / result.sum() # normalization, so that sum =1\n",
    "\n",
    "categorical_distribution(1, 2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate data, let's generate probabilities of transistions between observable states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_trans_probs = np.array(\n",
    "    [categorical_distribution(i, 2, len(observable_states)) for i in observable_states]\n",
    ")\n",
    "\n",
    "obs_initial_probs = np.ones(len(observable_states)) / len(observable_states)\n",
    "print('initial observations probabilities')\n",
    "print(obs_initial_probs)\n",
    "print()\n",
    "print('transitions between observations probabilities')\n",
    "print(obs_trans_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having those utilities, we can generate sequences of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(length, init_probs=obs_initial_probs, trans_probs=obs_trans_probs, n_categories=len(observable_states)):\n",
    "    result = []\n",
    "    result.append(np.random.choice(n_categories, p=init_probs))\n",
    "    for i in range(length -1):\n",
    "        result.append(np.random.choice(n_categories, p=obs_trans_probs[result[i]]))\n",
    "    \n",
    "    presence = [i in result for i in range(n_categories)]\n",
    "    return result if all(presence) else generate_sequence(length, init_probs, trans_probs, n_categories)\n",
    "\n",
    "generate_sequence(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_train = np.array([generate_sequence(10) for _ in range(1000)])\n",
    "observations_test = np.array([generate_sequence(10) for _ in range(10)])\n",
    "observations_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, all that's left is to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hmm.MultinomialHMM(n_components=len(hidden_states), n_iter=100)\n",
    "model = model.fit(observations_train)\n",
    "\n",
    "model.startprob_ = [0.5, 0.5]\n",
    "print('startprob')\n",
    "print(model.startprob_)\n",
    "print('transmat')\n",
    "print(model.transmat_)\n",
    "print('emissions')\n",
    "print(model.emissionprob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in observations_test:\n",
    "    logprob, decoded = model.decode(example.reshape(-1,1))\n",
    "    states = [list(hidden_states.keys())[d] for d in decoded]\n",
    "    print([z for z in zip(example, states)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
