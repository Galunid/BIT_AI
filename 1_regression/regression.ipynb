{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "We are going to dive into powerful universe of machine learning models. \n",
    "\n",
    "We will start with one of the easiest ones - linear regression. Though simple, it will introduce you to a number of important concepts, which are very much valid, when studying more sophisticated models such as neural networks.\n",
    "\n",
    "The idea of learning and intuition will be the same in almost all models, so make sure that you understand upcoming concepts first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "import sklearn as sk\n",
    "import solutions\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem of regression\n",
    "\n",
    "Consider two series' of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 3, 4, 5, 6, 7])\n",
    "Y = np.array([4.1, 6.7, 10.8, 14.3, 15.5, 20.0, 21.37])\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be clearly seen that there is a relationship between $X$ and $Y$. Moreover, this relationship is close to one of the simplest ones - it's linear. \n",
    "\n",
    "In other words:\n",
    "\n",
    "## $$y = w_0 + w_1 \\cdot x$$\n",
    "\n",
    "But how to find the good - or, as we'll say more often - **optimal** $w_0$ and $w_1$ for those two sets of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function \n",
    "\n",
    "Whenever you set yourself a goal, a good thing to figure out is how will you know you're satisfied (or not) with your results.\n",
    "\n",
    "In Machine Learning, the concept of **loss function** (or cost function) embodies this question. You can think of it as a metric that tells how satisfied you are with your solution. The better your model, the lower the loss.\n",
    "\n",
    "Of course, you have to evaluate your solution - or **model** in terms of the data you are interested in. So the loss function would have a form of\n",
    "\n",
    "### $$loss(model, input\\_data, output\\_data)$$\n",
    "\n",
    "or:\n",
    "\n",
    "### $$L(W, X, Y)$$\n",
    "\n",
    "Where:\n",
    "* $L$ - loss function\n",
    "* $W$ - the model\n",
    "* $X$ - the input data\n",
    "* $Y$ - the output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the model $W$ is simply the numbers $(w_0, w_1)$ we want to find. \n",
    "However, this won't always be such a simple case! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define the loss function!\n",
    "What do you think would be the best way to measure how well some $(w_0, w_1)$ capture the relationship between our $X$ and $Y$?\n",
    "\n",
    "Some important points to consider:\n",
    "* the better $(w_0, w_1)$ fit the actual data, the lower the loss\n",
    "* the loss shouldn't be dependent on the amount of the data, only on how well the model fits it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_loss(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray):\n",
    "    # implement your idea for loss function here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get stuck, check out the solutions script\n",
    "# where you will find both the naive and vectorized solutions\n",
    "# it's more rewarding to figure them out on your own, though!\n",
    "my_loss = solutions.my_loss\n",
    "my_loss = solutions.my_loss_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find the optimal $(w_0, w_1)$?\n",
    "\n",
    "Now that we have a way to measure the quality of our model, how can we find an optimal-enough one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_model(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray):\n",
    "    Y_pred = w_0 + w_1 * X \n",
    "    plt.scatter(X, Y)\n",
    "    plt.plot(X, Y_pred, 'r')\n",
    "    plt.show()\n",
    "    print('w_0:', w_0)\n",
    "    print('w_1:', w_1)\n",
    "    print('Loss:', my_loss(w_0, w_1, X, Y))\n",
    "    \n",
    "interact(plot_linear_model, \n",
    "         w_0=(-5.0, 5.0), \n",
    "         w_1=(-5.0,5.0),\n",
    "         X=fixed(X),\n",
    "         Y=fixed(Y)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytically\n",
    "\n",
    "As our loss function is not *that* complicated, one could use a least-squares method and  calculate it's derivative in terms of $w_0$ and $w_1$ and see which values minimize it. \n",
    "In this case, it would even work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1, w_0 = np.polyfit(X, Y, deg=1)\n",
    "plot_linear_model(w_0, w_1, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cases = 10\n",
    "w_0_space = np.linspace(0, 5, n_cases)\n",
    "w_1_space = np.linspace(5, 0, n_cases)\n",
    "loss_grid = np.zeros((n_cases, n_cases))\n",
    "for i in range(n_cases):\n",
    "    for j in range(n_cases):\n",
    "        w_1 = w_1_space[i]\n",
    "        w_0 = w_0_space[j]\n",
    "        loss_grid[i][j] = my_loss(w_0, w_1, X, Y)\n",
    "\n",
    "sns.heatmap(loss_grid, xticklabels=w_0_space, yticklabels=w_1_space, annot=loss_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in tougher cases, there would be much more than one global extrema and a much wider space of $W$ to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an ML method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching through the whole space of $W$ is computationally expensive. What if there was a technique to navigate through it more intelligently?\n",
    "\n",
    "Though normally there's no plausible way of generating a loss map such as above, we don't really need to know the loss throughout the whole space of solutions - we only want to find a place, where the loss will be lower than where we are currently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter Gradient Descent!\n",
    "\n",
    "If you know how to calculate the value of $L(w_0, w_1, X, Y)$ for a particular $(w_0, w_1)$, you can get a basic intuition about how that value is expected to change, should you shift $w_0$ or $w_1$ a bit. Do you know a math operation that does that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a simple derivative!\n",
    "\n",
    "Therefore, calculating the **values** of $\\dfrac{\\partial L}{\\partial w_0}$ and $\\dfrac{\\partial L}{\\partial w_1}$ **specifically at** $(w_0, w_1)$ tells you how the loss is expected to shift when $(w_0, w_1)$ will shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially, for any parameter $w$ if the value of $\\dfrac{\\partial L}{\\partial w}$ is positive, we suspect that increasing $w$ will lead to increase of $L$ and decreasing $w$ will lead to decrease in $L$. \n",
    "\n",
    "Another intuition is that the bigger absolute value of $\\dfrac{\\partial L}{\\partial w}$ is, the bigger (positive or negative) impact shifting of $w$ will have on $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the expressions for $\\dfrac{\\partial L}{\\partial w_0}$ and $\\dfrac{\\partial L}{\\partial w_1}$ ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dLdw0(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray):\n",
    "    pass\n",
    "\n",
    "def dLdw1(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dLdw0 = solutions.dLdw0\n",
    "dLdw1 = solutions.dLdw1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
