{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "We are going to dive into powerful universe of machine learning models. \n",
    "\n",
    "We will start with one of the easiest ones - linear regression. Though simple, it will introduce you to a number of important concepts, which are very much valid, when studying more sophisticated models such as neural networks.\n",
    "\n",
    "The idea of learning and intuition will be the same in almost all models, so make sure that you understand upcoming concepts first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "import sklearn as sk\n",
    "from typing import Tuple, List\n",
    "import solutions\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem of regression\n",
    "\n",
    "Consider two series' of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 3, 4, 5, 6, 7])\n",
    "Y = np.array([4.1, 6.7, 10.8, 14.3, 15.5, 20.0, 21.37])\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be clearly seen that there is a relationship between $X$ and $Y$. Moreover, this relationship is close to one of the simplest ones - it's linear. \n",
    "\n",
    "In other words:\n",
    "\n",
    "## $$y = w_0 + w_1 \\cdot x$$\n",
    "\n",
    "But how to find the good - or, as we'll say more often - **optimal** $w_0$ and $w_1$ for those two sets of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function \n",
    "\n",
    "Whenever you set yourself a goal, a good thing to figure out is how will you know you're satisfied (or not) with your results.\n",
    "\n",
    "In Machine Learning, the concept of **loss function** (or cost function) embodies this question. You can think of it as a metric that tells how satisfied you are with your solution. The better your model, the lower the loss.\n",
    "\n",
    "Of course, you have to evaluate your solution - or **model** in terms of the data you are interested in. So the loss function would have a form of\n",
    "\n",
    "### $$loss(model, input\\_data, output\\_data)$$\n",
    "\n",
    "or:\n",
    "\n",
    "### $$L(W, X, Y)$$\n",
    "\n",
    "Where:\n",
    "* $L$ - loss function\n",
    "* $W$ - the model\n",
    "* $X$ - the input data\n",
    "* $Y$ - the output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the model $W$ is simply the numbers $(w_0, w_1)$ we want to find. \n",
    "However, this won't always be such a simple case! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define the loss function!\n",
    "What do you think would be the best way to measure how well some $(w_0, w_1)$ capture the relationship between our $X$ and $Y$?\n",
    "\n",
    "Some important points to consider:\n",
    "* the better $(w_0, w_1)$ fit the actual data, the lower the loss\n",
    "* the loss shouldn't be dependent on the amount of the data, only on how well the model fits it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_loss(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "    # implement your idea for loss function here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you get stuck, check out the solutions script\n",
    "# where you will find both the naive and vectorized solutions\n",
    "# it's more rewarding to figure them out on your own, though!\n",
    "my_loss = solutions.my_loss\n",
    "my_loss = solutions.my_loss_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a sanity check, you can check \n",
    "# how your solutions with some random arguments and see \n",
    "# if it's consistent with provided solutions\n",
    "w_0 = np.random.rand()\n",
    "w_1 = np.random.rand()\n",
    "\n",
    "print('your solution:', my_loss(w_0, w_1, X, Y)) \n",
    "print('provided solution (naive)', solutions.my_loss(w_0, w_1, X, Y)) \n",
    "print('provided solution (vectorized)', solutions.my_loss_vectorized(w_0, w_1, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find the optimal $(w_0, w_1)$?\n",
    "\n",
    "Now that we have a way to measure the quality of our model, how can we find an optimal-enough one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_model(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray):\n",
    "    Y_pred = w_0 + w_1 * X \n",
    "    plt.scatter(X, Y)\n",
    "    plt.plot(X, Y_pred, 'r')\n",
    "    plt.show()\n",
    "    print('w_0:', w_0)\n",
    "    print('w_1:', w_1)\n",
    "    print('Loss:', my_loss(w_0, w_1, X, Y))\n",
    "    \n",
    "interact(plot_linear_model, \n",
    "         w_0=(-5.0, 5.0), \n",
    "         w_1=(-5.0,5.0),\n",
    "         X=fixed(X),\n",
    "         Y=fixed(Y)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytically\n",
    "\n",
    "As our loss function is not *that* complicated, one could use a least-squares method and  calculate it's derivative in terms of $w_0$ and $w_1$ and see which values minimize it. \n",
    "In this case, it would even work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1, w_0 = np.polyfit(X, Y, deg=1)\n",
    "plot_linear_model(w_0, w_1, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cases = 10\n",
    "w_0_space = np.linspace(0, 5, n_cases)\n",
    "w_1_space = np.linspace(5, 0, n_cases)\n",
    "loss_grid = np.zeros((n_cases, n_cases))\n",
    "for i in range(n_cases):\n",
    "    for j in range(n_cases):\n",
    "        w_1 = w_1_space[i]\n",
    "        w_0 = w_0_space[j]\n",
    "        loss_grid[i][j] = my_loss(w_0, w_1, X, Y)\n",
    "\n",
    "sns.heatmap(loss_grid, xticklabels=w_0_space, yticklabels=w_1_space, annot=loss_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in tougher cases, there would be much more than one global extrema and a much wider space of $W$ to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an ML method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching through the whole space of $W$ is computationally expensive. What if there was a technique to navigate through it more intelligently?\n",
    "\n",
    "Though normally there's no plausible way of generating a loss map such as above, we don't really need to know the loss throughout the whole space of solutions - we only want to find a place, where the loss will be lower than where we are currently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter Gradient Descent!\n",
    "\n",
    "The main idea of gradient descent is to repeatedly shift the weights of the model, until it finds a minimum of the loss function:\n",
    "\n",
    "![The idea of Gradient Descent](img/gradient_descent_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But how to shift weights?\n",
    "\n",
    "If you know how to calculate the value of $L(w_0, w_1, X, Y)$ for a particular $(w_0, w_1)$, you can also get a basic intuition about how that value is expected to change, should you shift $w_0$ or $w_1$ a bit from that point. \n",
    "\n",
    "Do you know a math operation that does that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's a simple derivative!\n",
    "\n",
    "Therefore, calculating the **values** of $\\dfrac{\\partial L}{\\partial w_0}$ and $\\dfrac{\\partial L}{\\partial w_1}$ **specifically at** $(w_0, w_1)$ tells you how the loss is expected to shift when $(w_0, w_1)$ will shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially, for any parameter $w$ if the value of $\\dfrac{\\partial L}{\\partial w}$ is positive, we suspect that increasing $w$ will lead to increase of $L$ and decreasing $w$ will lead to decrease in $L$. \n",
    "\n",
    "Another intuition is that the bigger absolute value of $\\dfrac{\\partial L}{\\partial w}$ is, the bigger (positive or negative) impact shifting of $w$ will have on $L$.\n",
    "\n",
    "![Gradient descent intuition](img/gradient_descent_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the expressions for $\\dfrac{\\partial L}{\\partial w_0}$ and $\\dfrac{\\partial L}{\\partial w_1}$ ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dLdw_0(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "    pass\n",
    "\n",
    "def dLdw_1(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dLdw_0 = solutions.dLdw_0\n",
    "dLdw_1 = solutions.dLdw_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a sanity check, you can check \n",
    "# how your solutions with some random arguments and see \n",
    "# if it's consistent with provided solutions\n",
    "w_0 = np.random.rand()\n",
    "w_1 = np.random.rand()\n",
    "\n",
    "print('your solution:', dLdw_0(w_0, w_1, X, Y), dLdw_1(w_0, w_1, X, Y)) \n",
    "print('provided solution ', solutions.dLdw_0(w_0, w_1, X, Y), dLdw_1(w_0, w_1, X, Y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are huge gradients! \n",
    "#### We now know in which direction to update the weights. But we still don't know - how much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = np.random.rand()\n",
    "w_1 = np.random.rand()\n",
    "w_0, w_1, my_loss(w_0, w_1, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient updates\n",
    "dw_0 = dLdw_0(w_0, w_1, X, Y)\n",
    "dw_1 = dLdw_1(w_0, w_1, X, Y)\n",
    "\n",
    "#update dradients\n",
    "w_0 -= dw_0 \n",
    "w_1 -= dw_1 \n",
    "\n",
    "#calculate new loss\n",
    "w_0, w_1, my_loss(w_0, w_1, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss has shot through the roof!\n",
    "\n",
    "That's why, when we perform our updates, we need to introduce some moderation. \n",
    "\n",
    "Updating the weights with the *exact* values of their respective gradients is never a good idea. \n",
    "\n",
    "That's why gradients are multiplied by a parameter called **learning rate** which tunes the speed of updating weights, so, in a way, decides how quickly your model learns.\n",
    "\n",
    "![Learning rates](img/learning_rate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are ready to implement a simple linear regressor!\n",
    "\n",
    "First, write a function that will perform a single gradient step:\n",
    "* calculate the loss for the given $(w_0, w_1)$\n",
    "* calculate $\\dfrac{\\partial L}{\\partial w_0}$ and $\\dfrac{\\partial L}{\\partial w_1}$\n",
    "* calculate updated $(w_0, w_1)$ according to calculated gradients and the learning_rate\n",
    "* return updated $(w_0, w_1)$ and the calculated loss for their previous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hint - my_loss, dLdw0 and dLdw1 should be helpful here!\n",
    "def gradient_step(\n",
    "    w_0: float, \n",
    "    w_1: float, \n",
    "    X: np.ndarray, \n",
    "    Y: np.ndarray, \n",
    "    learning_rate: float\n",
    ") -> Tuple[float, float, float]:\n",
    "    return 0, 0, 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you get stuck, check out the solutions script\n",
    "# where you will find both the naive and vectorized solutions\n",
    "# it's more rewarding to figure them out on your own, though!\n",
    "gradient_step = solutions.gradient_step_naive\n",
    "gradient_step = solutions.gradient_step_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a sanity check, you can check \n",
    "# how your solutions with some random arguments and see \n",
    "# if it's consistent with provided solutions\n",
    "\n",
    "w_0 = np.random.rand()\n",
    "w_1 = np.random.rand()\n",
    "learning_rate = 0.1\n",
    "\n",
    "print('your solution', gradient_step(w_0, w_1, X, Y, learning_rate))\n",
    "print('provided solution (naive)', \n",
    "      solutions.gradient_step_naive(w_0, w_1, X, Y, learning_rate))\n",
    "print('provided solution (vectorized)', \n",
    "      solutions.gradient_step_vectorized(w_0, w_1, X, Y, learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're ready to train a first model!\n",
    "\n",
    "To find the optimal $(w_0, w_1)$, now write a function which, given initial weights, data and the learning rate will perform the gradient step a number of times and return the final $(w_0, w_1)$.\n",
    "\n",
    "Observing how the loss changes throughtout the training can often provide an invaluable insight on how well the model is working.\n",
    "\n",
    "In order to later visualize the loss, return a list of calculated losses for each iteration as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    init_w_0: float,\n",
    "    init_w_1: float,\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    learning_rate: float,\n",
    "    num_iterations: int\n",
    ") -> Tuple[float, float, List[float]]:\n",
    "    return 0, 0, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you get stuck, check out the solutions script\n",
    "# where you will find both the naive and vectorized solutions\n",
    "# it's more rewarding to figure them out on your own, though!\n",
    "\n",
    "train_model = solutions.train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_w_0 = np.random.rand()\n",
    "init_w_1 = np.random.rand()\n",
    "learning_rate = 1\n",
    "num_iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_w_0, trained_w_1, loss_history = \\\n",
    "    train_model(init_w_0, init_w_1, X, Y, learning_rate, num_iterations)\n",
    "\n",
    "plt.plot(list(range(num_iterations)), loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_model(trained_w_0, trained_w_1, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the default `learning rate` and `num_iterations` shouldn't laern very well. Try experimenting with various values and see if you can find their combination for which the model will train nicely!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You've just coded your first Machine Learning model!\n",
    "\n",
    "Now play with it as you wish!\n",
    "* what happens when you change learning rate?\n",
    "* what about the number of iterations?\n",
    "* how would the model behave if trained on a more complex dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hard = solutions.X_hard\n",
    "Y_hard = solutions.Y_hard\n",
    "plt.scatter(X_hard, Y_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feel free to play with the dataset here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - linear regression with scikit-learn\n",
    "\n",
    "As you may have already guessed, there are python liobraries which provide higher-level API for various machine Learning problems. One with of the richest (though not the best for real-life applications) is scikit-learn (http://scikit-learn.org/).\n",
    "\n",
    "It provides implementations for various simple algorithms, useful in ML and Data Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_2d = X.reshape(-1, 1) \n",
    "# input to the model must be of shape [n_examples, 1] \n",
    "# why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X_2d, Y, )\n",
    "Y_pred = regressor.predict(X_2d)\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X, Y_pred, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the actual weights the model has learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_w_1 = regressor.coef_[0]\n",
    "sk_w_0 = regressor.intercept_\n",
    "sk_w_0, sk_w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what are the weights learnt by our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_w_0, trained_w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
