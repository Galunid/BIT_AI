{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick rewind on the process of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, we'll load the data and create one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_wine()\n",
    "dataset = datasets.load_iris()\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, shuffle it and split into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ind = np.arange(y.shape[0])\n",
    "np.random.shuffle(rand_ind)\n",
    "\n",
    "X = X[rand_ind]\n",
    "y = y[rand_ind]\n",
    "\n",
    "X = X / X.max(axis=0) # normalize dataset\n",
    "\n",
    "split = 100\n",
    "\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]\n",
    "\n",
    "X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a model. It will be a two-layer fully connected neural network with ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_model(d_in=X.shape[1], d_hidden=20, d_out=(y.max() + 1)):\n",
    "    print(d_in, d_hidden, d_out)\n",
    "    model = {}\n",
    "    # first layer\n",
    "    model[\"W1\"] = np.random.rand(d_in, d_hidden) #weights\n",
    "    model[\"b1\"] = np.zeros(d_hidden) # biases\n",
    "\n",
    "    # second layer\n",
    "    model[\"W2\"] = np.random.rand(d_hidden, d_out) #weights\n",
    "    model[\"b2\"] = np.zeros(d_out) # biases\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remember ReLU activation?\n",
    "def relu(activation):\n",
    "    return activation * (activation > 0)\n",
    "\n",
    "def accuracy(y_pred, y_actual):\n",
    "    accurate = (np.argmax(y_pred, axis=1) == np.argmax(y_actual, axis=1)).sum()\n",
    "    return accurate / y_pred.shape[0]\n",
    "\n",
    "def to_onehot(y_labels):\n",
    "    onehot = np.zeros((y_labels.shape[0], y_labels.max() + 1))\n",
    "    onehot[np.arange(y_labels.shape[0]), y_labels] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass through the network looks like this.\n",
    "\n",
    "As you can see, a forward pass through fully connected network of a layer is nothing more than multiplication of input by some matrix and addition of a bias vector to the result!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(model, X):\n",
    "    W1, b1, W2, b2 = model[\"W1\"], model[\"b1\"], model[\"W2\"], model[\"b2\"]\n",
    "    \n",
    "    a1 = X\n",
    "    #forward pass through layer 1\n",
    "    o1 = a1 @ W1 + b1\n",
    "    # pass output 1 through ReLU to compute activation 2\n",
    "    a2 = relu(o1)\n",
    "    # pass activation 2 through layer 2\n",
    "    o2 = a2 @ W2 + b2\n",
    "    # output 2 are our predictions!\n",
    "    y_pred = o2\n",
    "    # we're going to need intermediate values if from the forward pass\n",
    "    # in training the model - hence cache\n",
    "    cache = a1, o1, a2, o2\n",
    "    \n",
    "    return y_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define what happens during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, X=X, y=y, lr=1e-3):\n",
    "    # a matrix of one-hot vectors representing the labels\n",
    "    # example: if datapoint has label 2, the corresponding one-hot vector will be [0,1,0]\n",
    "    W1, b1, W2, b2 = model[\"W1\"], model[\"b1\"], model[\"W2\"], model[\"b2\"]\n",
    "\n",
    "    # first, predictions need to be made\n",
    "    y_pred, cache = forward(model, X)\n",
    "    \n",
    "    a1, o1, a2, o2 = cache\n",
    "    \n",
    "    # we'll compute loss using softmax loss funtion\n",
    "    # http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/\n",
    "    XE = np.exp(y_pred)\n",
    "    true_indices = np.arange(y.shape[0]), y\n",
    "    true_XE = XE[true_indices]\n",
    "    XE_sums = XE.sum(axis=1)\n",
    "    \n",
    "    loss = (-np.log(true_XE / XE_sums)).sum() / X.shape[0]\n",
    "    \n",
    "    y_onehot = to_onehot(y)\n",
    "    acc = accuracy(y_pred, y_onehot)\n",
    "    \n",
    "    # gradient of cost with respect to y_pred\n",
    "    # = how should y_pred change in order for cost to be greater?\n",
    "    do2 = ((XE.T / XE_sums).T - y_onehot) / X.shape[0]\n",
    "    # backpropagation of gradient through layer 2\n",
    "    # compute gradient with respect to W2, b2, and activation_2\n",
    "    dW2 = a2.T @ do2 \n",
    "    db2 = do2.sum(axis=0)\n",
    "    da2 = do2 @ W2.T\n",
    "    \n",
    "    # propagation through relu\n",
    "    do1 = da2 * (o1 > 0)\n",
    "    \n",
    "    # backpropagation through layer 1\n",
    "    dW1 = a1.T @ do1\n",
    "    db1 = do1.sum(axis=0)\n",
    "    \n",
    "    # this is gradient with respect to the input \n",
    "    # - we don't actually need it\n",
    "    da1 = do1 @ W1.T\n",
    "    \n",
    "    model[\"W2\"] -= lr * dW2\n",
    "    model[\"b2\"] -= lr * db2\n",
    "    model[\"W1\"] -= lr * dW1\n",
    "    model[\"b1\"] -= lr * db1\n",
    "    \n",
    "    return loss, acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see that in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_hist = []\n",
    "acc_hist = []\n",
    "model = new_model(d_hidden=20)\n",
    "iters = 1000\n",
    "for i in range(iters):\n",
    "    loss, acc = train(model, lr=1e-2)\n",
    "    cost_hist.append(loss)\n",
    "    acc_hist.append(acc)\n",
    "\n",
    "print('cost')\n",
    "plt.plot(np.arange(iters), cost_hist)\n",
    "plt.show()\n",
    "print('accuracy')\n",
    "plt.plot(np.arange(iters), acc_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what's the accuracy on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, _ = forward(model, X_test)\n",
    "accuracy(y_pred, to_onehot(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
