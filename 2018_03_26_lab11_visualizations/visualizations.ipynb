{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with network visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we won't train any models. Instead, we'll work with a pre-trained model called SqueezeNet.\n",
    "https://github.com/DeepScale/SqueezeNet\n",
    "\n",
    "It's a classifier trained on the ImageNet dataset which is accurate and also computationally quite lighweight. Because of that, it's possible to run it on CPU and not want to kill oneself ;)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First layer visualization\n",
    "\n",
    "The first of methods we'll discuss today will involve visualizing the weights in the first convolutional layer of the network. \n",
    "\n",
    "This is possible, since the first layer interacts directly with the images. For the next layers, which interact with more abstract (and complex) outputs of the previous layers, such visualization is not possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer = list(model.parameters())[0]\n",
    "first_layer.size()\n",
    "\n",
    "# 64 filters that interact with 3x3 patches of pixels (RGB values, hence the depth of the channel is also 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i, flt in enumerate(first_layer):\n",
    "    plt.subplot(8, 8, i+1)\n",
    "    plt.imshow(flt.data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very verbose, is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency maps\n",
    "\n",
    "In the next technique we'll try to figure out which parts of the image made the biggest impact on the classification decision of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model has been trained on ImageNet dataset ( http://www.image-net.org/ ), we'll load some sample pictures from the dataset. \n",
    "\n",
    "As you can see, the dataset is split into 1000 classes! Wow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenet(download=False):\n",
    "    imagenet_file = 'imagenet_val_25.npz'\n",
    "    if download: subprocess.call(['wget', 'http://cs231n.stanford.edu/' + imagenet_file])\n",
    "    f = np.load(imagenet_file)\n",
    "    X = f['X']\n",
    "    y = f['y']\n",
    "    class_names = f['label_map'].item()\n",
    "    return X, y, class_names\n",
    "\n",
    "X, y, class_names = load_imagenet()\n",
    "class_names\n",
    "# datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imagenet(i=0):\n",
    "    pic = X[i]\n",
    "    name = class_names[y[i]]\n",
    "    print(name)\n",
    "    plt.imshow(pic)\n",
    "    plt.show()\n",
    "\n",
    "show_imagenet(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once we're done with gazing at the dataset, let's wrap the data into `torch.Variables` and get to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_mean = np.array([0.485, 0.456, 0.406])\n",
    "X_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# X_var = X.copy()\n",
    "X_var = (X - X_mean) / X_std\n",
    "X_var = X_var.transpose(0, 3, 1, 2)\n",
    "X_var = Variable(torch.FloatTensor(X_var), requires_grad=True)\n",
    "y_var = Variable(torch.LongTensor(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salency maps are computed in an algorithm quite similiar to backpropagation. \n",
    "\n",
    "In backpropagation, we computed the gradients of loss with respect to weight matrices. In other words, we asked - how much the change of each weight would affect the loss function?\n",
    "\n",
    "In the case of computing saliency maps, we'll also compute a gradient. It will be a gradient of the strength of classification as the desired class with respect to the input image. \n",
    "\n",
    "In other words - how much the change of which pixels affects the output classification? \n",
    "Which is precisely what we want to know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saliency_maps(X_var, y_var, model):\n",
    "    # create a variable of one-hot vectors based on y_var (ground-truth labels)\n",
    "    y_onehot = np.zeros((y_var.size()[0], y_var.max().data.numpy()[0] + 1))\n",
    "    y_onehot[np.arange(y_var.size()[0]), y_var.data] = 1\n",
    "    y_onehot = Variable(torch.FloatTensor(y_onehot), requires_grad=False)\n",
    "    \n",
    "    # compute classifications \n",
    "    y_pred = model(X_var)\n",
    "    \n",
    "    # compute gradients\n",
    "    # y_onehot serves as an initial gradient \n",
    "    # 0s for wrong classes, 1s for the right classes\n",
    "    # this way, we effectively compute only the gradient \n",
    "    # of the classification strength of the right class\n",
    "    y_pred.backward(y_onehot)\n",
    "  \n",
    "    # backprop from ground-truth scores with initial gradients == 1\n",
    "    \n",
    "    # extracting the gradients with respect to inputs\n",
    "    saliency = X_var.grad.data\n",
    "    # to see which gradients are big, we'll consider \n",
    "    # their absolute values\n",
    "    saliency = saliency.abs() \n",
    "    # each pixel has actually three values of gradient computed \n",
    "    # - with respect to each color channel\n",
    "    # we'll consider only the biggest one\n",
    "    saliency, _ = torch.max(saliency, dim=1)\n",
    "    return saliency\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the saliencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saliencies = saliency_maps(X_var, y_var, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's see wht tips the network off about the contents of the image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, s) in enumerate(zip(X, saliencies)):\n",
    "    print(y[i], class_names[y[i]])\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(x)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(s, cmap=plt.cm.hot)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class visualization\n",
    "\n",
    "We computed the gradient of class hypothesis with respect to the image in order to see which pixels in the image matter the most.\n",
    "\n",
    "Gradients are normally used during training to teach the (initially random) model to fit to our expectations. Perhaps we could \"train\" an image in a similar way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the process, we'll also blur the image to make it look more natural\n",
    "def blur_image(X, sigma=1):\n",
    "    X_np = X.cpu().clone().numpy()\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n",
    "    X.copy_(torch.Tensor(X_np).type_as(X))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_class_visualization(target_y, model, blur_every=10, learning_rate=25, l2_reg=1e-3, num_iterations=200, show_every=50):\n",
    "    # initialize some random noise\n",
    "    img = torch.randn(1, 3, 224, 224).mul_(1.0)\n",
    "    img_var = Variable(img, requires_grad=True)\n",
    "\n",
    "    for t in range(num_iterations):\n",
    "        # make predictions\n",
    "        y_pred = model(img_var)\n",
    "        # get the score of the class we want to visualize\n",
    "        target_score = y_pred[0, target_y]\n",
    "        # compute the gradient of that score with respect to the image\n",
    "        target_score.backward()\n",
    "        dimg = img_var.grad.data \n",
    "        \n",
    "        # apply regularization\n",
    "        dimg -= 2 * l2_reg * img_var.data\n",
    "        img_var.data += learning_rate * (dimg / dimg.norm())\n",
    "    \n",
    "        # manually zero the gradient\n",
    "        img_var.grad.data.zero_()\n",
    "        \n",
    "    \n",
    "        # as regularizer, clamp and periodically blur the image\n",
    "        for c in range(3):\n",
    "            lo = float(-X_mean[c] / X_std[c])\n",
    "            hi = float((1.0 - X_mean[c]) / X_std[c])\n",
    "            img[:, c].clamp_(min=lo, max=hi)\n",
    "        \n",
    "        if t % blur_every == 0:\n",
    "            blur_image(img, sigma=0.5)\n",
    "\n",
    "        # periodically show the image\n",
    "        if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1:\n",
    "            to_show = img.numpy()[0].transpose(1, 2, 0)\n",
    "            to_show = (to_show * X_std) + X_mean\n",
    "            plt.imshow(to_show)\n",
    "            class_name = class_names[target_y]\n",
    "            plt.title('%s\\nIteration %d / %d' % (class_name, t + 1, num_iterations))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_class_visualization(817, model, blur_every=2, l2_reg=5e-3, show_every=125, num_iterations=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not a Tesla, but still looks like something ridable. At the very least, it has wheels!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
