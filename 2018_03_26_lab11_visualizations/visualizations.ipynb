{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with network visualizations\n",
    "\n",
    "Agenda:\n",
    "\n",
    "* First layer visualization\n",
    "* Saliency maps\n",
    "* Class visualization\n",
    "* DeepDream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we won't train any models. Instead, we'll work with a pre-trained model called SqueezeNet.\n",
    "https://github.com/DeepScale/SqueezeNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First layer visualization\n",
    "\n",
    "The first of methods we'll discuss today will involve visualizing the weights in the first convolutional layer of the network. \n",
    "\n",
    "This is possible, since the first layer interacts directly with the images. For the next layers, which interact with more abstract (and complex) outputs of the previous layers, such visualization is not possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer = list(model.parameters())[0]\n",
    "first_layer.size()\n",
    "\n",
    "# 64 filters that interact with 3x3 patches of pixels (RGB values, hence the depth of the channel is also 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i, flt in enumerate(first_layer):\n",
    "    plt.subplot(8, 8, i+1)\n",
    "    plt.imshow(flt.data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very verbose, is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency maps\n",
    "\n",
    "In the next technique we'll try to figure out which parts of the image made the biggest impact on the classification decision of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model has been trained on ImageNet dataset ( http://www.image-net.org/ ), we'll load some sample pictures from the dataset. \n",
    "\n",
    "As you can see, the dataset is split into 1000 classes! Wow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenet(download=False):\n",
    "    imagenet_file = 'imagenet_val_25.npz'\n",
    "    if download: subprocess.call(['wget', 'http://cs231n.stanford.edu/' + imagenet_file])\n",
    "    f = np.load(imagenet_file)\n",
    "    X = f['X']\n",
    "    y = f['y']\n",
    "    class_names = f['label_map'].item()\n",
    "    return X, y, class_names\n",
    "\n",
    "X, y, class_names = load_imagenet()\n",
    "class_names\n",
    "# datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imagenet(i=0):\n",
    "    pic = X[i]\n",
    "    name = class_names[y[i]]\n",
    "    print(name)\n",
    "    plt.imshow(pic)\n",
    "    plt.show()\n",
    "\n",
    "show_imagenet(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once we're done with gazing at the dataset, let's wrap the data into `torch.Variables` and get to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.array([0.485, 0.456, 0.406])\n",
    "X_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# X_var = X.copy()\n",
    "X_var = (X - X_mean) / X_std\n",
    "X_var = X_var.transpose(0, 3, 1, 2)\n",
    "X_var = Variable(torch.FloatTensor(X_var), requires_grad=True)\n",
    "y_var = Variable(torch.LongTensor(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salency maps are computed in an algorithm quite similiar to backpropagation. \n",
    "\n",
    "In backpropagation, we computed the gradients of loss with respect to weight matrices. In other words, we asked - how much the change of each weight would affect the loss function?\n",
    "\n",
    "In the case of computing saliency maps, we'll also compute a gradient. It will be a gradient of the strength of classification as the desired class with respect to the input image. \n",
    "\n",
    "In other words - how much the change of which pixels affects the output classification? \n",
    "Which is precisely what we want to know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saliency_maps(X_var, y_var, model):\n",
    "    # create a variable of one-hot vectors based on y_var (ground-truth labels)\n",
    "    y_onehot = np.zeros((y_var.size()[0], y_var.max().data.numpy()[0] + 1))\n",
    "    y_onehot[np.arange(y_var.size()[0]), y_var.data] = 1\n",
    "    y_onehot = Variable(torch.FloatTensor(y_onehot), requires_grad=False)\n",
    "    \n",
    "    # compute classifications \n",
    "    y_pred = model(X_var)\n",
    "    \n",
    "    # compute gradients\n",
    "    # y_onehot serves as an initial gradient \n",
    "    # 0s for wrong classes, 1s for the right classes\n",
    "    # this way, we effectively compute only the gradient \n",
    "    # of the classification strength of the right class\n",
    "    y_pred.backward(y_onehot)\n",
    "  \n",
    "    # backprop from ground-truth scores with initial gradients == 1\n",
    "    \n",
    "    # extracting the gradients with respect to inputs\n",
    "    saliency = X_var.grad.data\n",
    "    # to see which gradients are big, we'll consider \n",
    "    # their absolute values\n",
    "    saliency = saliency.abs() \n",
    "    # each pixel has actually three values of gradient computed \n",
    "    # - with respect to each color channel\n",
    "    # we'll consider only the biggest one\n",
    "    saliency, _ = torch.max(saliency, dim=1)\n",
    "    return saliency\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the saliencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliencies = saliency_maps(X_var, y_var, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's see wht tips the network off about the contents of the image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, s) in enumerate(zip(X, saliencies)):\n",
    "    print(class_names[y[i]])\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(x)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(s, cmap=plt.cm.hot)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
