{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to dive into powerful universe of machine learning models. We will start with the easiest one. The idea of learning and intuition will be the same in almost all models, so make sure that you understand upcoming concept first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$\\hat{y} = w_0 + w_1x_1+ w_2x_2+ w_3x_3+ ... + w_nx_n$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$\\hat{y} = w_0 + w_1x_1$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 1,

   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "import andrzej\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My favorite numbers"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 2,

   "metadata": {},
   "outputs": [],
   "source": [
    "x = andrzej.x\n",
    "y = andrzej.y\n",
    "# y = andrzej.y_hard"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4DczqxkHfjOzmnHgNzOrGQd+M7Oa+X+P7hTV2WjqJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce10cab128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],

   "source": [
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 4,

   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_model(a, b):\n",
    "    y_pred = a * x + b\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, y_pred, 'r')\n",
    "    plt.xlim([x.min() - 5, x.max() + 5])\n",
    "    plt.ylim([y.min() - 5, y.max() + 5])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='a', max=5.0, min=-5.0), FloatSlider(value=0.0, description='b', max=5.0, min=-5.0), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_linear_model>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],

   "source": [
    "interact(plot_linear_model, a=(-5.0, 5.0), b=(-5.0,5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$L = \\frac{1}{N}\\sum_{i=0}^N(\\hat{y_i} - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$\\epsilon_i = \\hat{y_i} - y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of cost function is estimating how good our $w_i$ are. The \"better\" the $w_i$ the lower the loss/cost.\n",
    "In regression the most used cost is _least squares_. https://en.wikipedia.org/wiki/Least_squares\n",
    "\n",
    "The $\\epsilon_i$ represents the error of our prediction with respect to ground truth value. This is also called _residual_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x, y, w_0, w_1):\n",
    "    y_pred = w_0 + w_1 * x\n",
    "    error = y - y_pred\n",
    "    squared_error = error ** 2\n",
    "    return squared_error.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(cost, w_0=(-5.0, 5.0), w_1=(-5.0, 5.0), x=fixed(x), y=fixed(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem of today's workshop is optimization. There exist many approaches that try to solve it. Optimization is the process of finding the best _paramters_ with respect to some _value_. It is mainly finding _parameters_ that minimize/maximize the _value_.\n",
    "\n",
    "We fill focus on finding the $w_i$ that minimize $L$ (we will use only $w_0$ and $w_1$).\n",
    "\n",
    "One approach could be: choose let's say 10 000 different values of $w_0$ and $w_1$ and pick the values that minimize $L$.\n",
    "\n",
    "More clever approach is solving the equation _analytically_. It will give you the _exact solution_. In linear regression it is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However there exist another family of algorithm that optimize a function and do it _numerically_. The main algorithm is _gradient descent_. The main idea of _GD_ is changing the $w_i$ by really small value in a direction that minimizes $L$.\n",
    "\n",
    "It's an iterative algorithm. It means that you repeat the step explained before until you are satisfied with your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good source explaining gradient descent: http://lmgtfy.com/?q=gradient+descent\n",
    "\n",
    "There are many videos/posts explaining that algorithm. Here is script from _CS229: Machine Learning_ course at Stanford University (taught previously by Andrew Ng). http://cs229.stanford.edu/notes/cs229-notes1.pdf\n",
    "\n",
    "Whole course: http://cs229.stanford.edu/syllabus.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\epsilon = \\frac{1}{N}\\sum_{i}^N \\epsilon_i $$\n",
    "$$ k = \\frac{1}{N}\\sum_{i}^N \\epsilon_i x_i $$\n",
    "$$ w_0 = w_0 - 2 \\alpha \\epsilon $$\n",
    "$$ w_1 = w_1 - 2 \\alpha k $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that you need to update $w_0$ and $w_1$ simultaneously. You shouldn't update $w_0$ and then use it too calculate $k$. You should compute $\\epsilon$ and $k$ before and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(x, y, w_0, w_1, alpha=0.01):\n",
    "    # TO BE IMPLEMENTED\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_step = andrzej.gradient_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_step(x, y, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_0=0.0, w_1=0.0, alpha=0.01, steps=1000, history=True, verbose=True):\n",
    "    w_history = [(w_0, w_1)]\n",
    "    cost_history = []\n",
    "    for i in range(steps):\n",
    "        current_cost =  cost(x, y, w_0, w_1)\n",
    "        if verbose and i%100 == 0:\n",
    "            print(\"Current cost: \", current_cost)\n",
    "        cost_history.append(current_cost)\n",
    "        \n",
    "        w_0, w_1 = gradient_step( x, y, w_0, w_1,alpha)\n",
    "        w_history.append((w_0, w_1))\n",
    "    if not history:\n",
    "        return w_0, w_1\n",
    "    else:\n",
    "        return w_0, w_1, w_history, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0, w_1, w_history, cost_history = gradient_descent(x, y, alpha=0.001, steps=30, history=True, verbose=False)\n",
    "print(\"w_0: {}\".format(w_0))\n",
    "print(\"w_1: {}\".format(w_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(cost_history)), cost_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient_descent(idx, history):\n",
    "    w_0, w_1 = history[idx]\n",
    "    y_pred = w_1 * x + w_0\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, y_pred, 'r')\n",
    "    plt.xlim([x.min() - 5, x.max() + 5])\n",
    "    plt.ylim([y.min() - 5, y.max() + 5])\n",
    "    plt.show()\n",
    "    print(\"w_0: {}\".format(w_0))\n",
    "    print(\"w_1: {}\".format(w_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_gradient_descent, idx=(0, len(w_history)-1), history=fixed(w_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can compare your solutions to one's provided by most used library in machine learning, which is _scikit-learn_. In day to day work you normally don't implement algorithms like _linear regression_, but you should now how it is implemented to use it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x.reshape(-1, 1), y) # X in sklearn must be two-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"your w_0: {}\".format(w_0))\n",
    "print(\"your w_1: {}\".format(w_1))\n",
    "\n",
    "print(\"sklearn w_0: {}\".format(model.intercept_))\n",
    "print(\"sklearn w_1: {}\".format(model.coef_[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
