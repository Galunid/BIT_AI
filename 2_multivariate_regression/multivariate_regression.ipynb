{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "Today we're going to talk some more about last-time's topic - **Linear Regression** in a more generalized way. \n",
    "\n",
    "You'll see that with just a few changes, we will be able to apply Linear Regression model to problems much more interesting than plotting straight lines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "import solutions\n",
    "import importlib.util\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previously\n",
    "\n",
    "### $$y = w_0 + w_1 \\cdot x$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = solutions.X_1\n",
    "Y = solutions.Y_1\n",
    "\n",
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we want to find the optimal $(w_0, w_1)$ - our **model**\n",
    "\n",
    "* our model will be able to make a **hypothesis**\n",
    "\n",
    "### $$ h_W(x) = w_0 + w_1 \\cdot x$$\n",
    "\n",
    "* a **loss function** lets as calculate how well our model **fits** the data. In this case, the loss function could look like this:\n",
    "\n",
    "### $$L_{prev} = \\frac{1}{N}\\sum_{i=0}^N(h_W(x^{(i)}) - y^{(i)})^2 $$\n",
    " \n",
    " Though it's good enough for our purposes, we'll divide loss function by 2 - it won't change anything about it illustrating the quality of the model, but will simplify latter computations (can you guess how?)\n",
    " \n",
    "### $$L = \\frac{1}{2N}\\sum_{i=0}^N(h_W(x^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "* We can find $(w_0, w_1)$ by using **Gradient Descent** method. \n",
    "\n",
    "* If we calculate gradients of $L$ with respect to $(w_0, w_1)$, or $\\dfrac{\\partial L}{\\partial w_0}$ and $\\dfrac{\\partial L}{\\partial w_1}$, we will know how to shift the values of $(w_0, w_1)$, so that they will fit the data better.\n",
    "\n",
    "### $$\n",
    "\\dfrac{\\partial L}{\\partial w_0} = \\dfrac{\\sum_{i=0}^n w_0 + w_1 x^{(i)} - y^{(i)}}{n} \\\\\n",
    "\\dfrac{\\partial L}{\\partial w_1} =  \\dfrac{\\sum_{i=0}^n (w_0 + w_1 x^{(i)} - y^{(i)}) \\cdot x^{(i)}}{n}\n",
    "$$\n",
    "\n",
    "![The idea of Gradient Descent](../1_regression/img/gradient_descent_0.png)\n",
    "\n",
    "* We multiply the gradients by a **learning rate** $\\alpha$, so that the updates are small and don't overshoot their objective.\n",
    "\n",
    "### $$\n",
    "w_0 = w_0 - \\dfrac{\\partial L}{\\partial w_0} \\cdot \\alpha \\\\\n",
    "w_1 = w_1 - \\dfrac{\\partial L}{\\partial w_1} \\cdot \\alpha\n",
    "$$\n",
    "* We repeat that process for an arbitrary number of **epochs**\n",
    "\n",
    "![Learning rates](../1_regression/img/learning_rate.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"solutions\", \n",
    "    \"../1_regression/solutions.py\"\n",
    ")\n",
    "solutions_1 = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(solutions_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w_0 = np.random.rand()\n",
    "init_w_1 = np.random.rand()\n",
    "learning_rate = 0.01\n",
    "num_iterations = 100\n",
    "\n",
    "trained_w_0, trained_w_1, loss_history = \\\n",
    "    solutions_1.train_model(init_w_0, init_w_1, X, Y, learning_rate, num_iterations)\n",
    "\n",
    "plt.plot(list(range(num_iterations)), loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = trained_w_0 + trained_w_1 * X \n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X, Y_pred, 'r')\n",
    "plt.show()\n",
    "print('w_0:', trained_w_0)\n",
    "print('w_1:', trained_w_1)\n",
    "print('Loss:', solutions_1.my_loss_vectorized(trained_w_0, trained_w_1, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's go bigger!\n",
    "\n",
    "Today, we'll apply linear regression to real-life data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset.data)\n",
    "df.columns = dataset.feature_names\n",
    "df['target'] = dataset.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're dealing with a dataset describing houses - each of them has **13** features. Let's see how each of them is related to our target - the price of the house!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in df.columns:\n",
    "    if feature != 'target':\n",
    "        print(f\"-------{feature}--------\")\n",
    "        plt.scatter(df[feature], df['target'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the plots, in most of them some kind of relationship can be observed.\n",
    "\n",
    "Now, is it possible to use what we already know to train a model which will make accurate enough predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference is that previously:\n",
    "\n",
    "### $$\\hat{y} = h_W(x) = w_0 + w_1x$$ \n",
    "\n",
    "And today:\n",
    "\n",
    "### $$\\hat{y} = h_W(x_1, x_2, ..., x_k) \\\\\n",
    "= w_0 + w_1x_1+ w_2x_2+ w_3x_3+ ... + w_kx_k \\\\\n",
    "= w_0 + \\sum_{i=1}^k w_i x_i$$ \n",
    "\n",
    "As you can see, $w_0$ has been left out from the sum, which makes it sad. Can we do something, which will make it possible to include it there?\n",
    "\n",
    "The simple solution is to add a *bias feature* to our input dataset - $X$ - i.e. add a column of ones to it.\n",
    "\n",
    "This way, for each datapoint $x^{(j)}$,  \n",
    "### $$x_0^{(j)} = 1$$\n",
    "and \n",
    "### $$ x_0^{(j)} \\cdot w_0 = w_0 $$\n",
    "\n",
    "therefore\n",
    "\n",
    "### $$ w_0 + \\sum_{i=1}^k w_i x_i^{(j)} =  \\sum_{i=0}^k w_i x_i^{(j)} = h_W(x^{(j)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating this manually every time is not a goood idea. \n",
    "\n",
    "Your task now is to implement a function which will compute the hypotheses for given data $(X)$ and model $(w_0, w_1, w_2, w_3, w_4, w_5, w_6, w_7, w_8, w_9, w_{10}, w_{11}, w_{12}, w_{13})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hypothesis(\n",
    "    X: np.ndarray,\n",
    "    w_0: float, \n",
    "    w_1: float, \n",
    "    w_2: float, \n",
    "    w_3: float, \n",
    "    w_4: float, \n",
    "    w_5: float, \n",
    "    w_6: float, \n",
    "    w_7: float, \n",
    "    w_8: float, \n",
    "    w_9: float, \n",
    "    w_10: float, \n",
    "    w_11: float, \n",
    "    w_12: float, \n",
    "    w_13: float\n",
    ") -> np.ndarray:\n",
    "    pass\n",
    "    # PLEASE DON'T EVER DO THAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, I got tired of even writing this header!\n",
    "\n",
    "We obviously need something more elegant. This is why, from now on, we'll always think of particular datapoints not as numbers, but vectors of numbers. Therefore, the whole dataset will be a **vector of vectors** - a matrix.\n",
    "\n",
    "The same way, we won't care much for every particular weight in our model, we'll treat them as a single vector of numbers.\n",
    "\n",
    "So: \n",
    "### $$\n",
    "\\hat{y}^{(j)} = h_W(x^{(j)}) \\sum_{i=0}^k w_i x_i^{(j)} = \\sum Wx^{(j)} \n",
    "$$\n",
    "\n",
    "* $W$ has shape $[n_{features}]$\n",
    "* $X$ has shape $[n_{datapoints}, n_{features}]$\n",
    "* $Y$ has shape $[n_{datapoints}]$\n",
    "\n",
    "Please implement it. If you use numpy magic instead of iterating over columns, it should take you just one line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hypotheses(W: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "    return np.zeros(X.size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypotheses = solutions.hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a sanity check on a few examples!\n",
    "W = np.random.rand(X.shape[1])\n",
    "print('your solution:', hypotheses(W, X[:5]))\n",
    "print('provided solution:', solutions.hypotheses(W, X[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also means we have to update the formula for the cost function:\n",
    "\n",
    "### $$\n",
    "L(w_0, w_1, ... w_n) = L(W) \\\\ \n",
    "= \\frac{1}{2N}\\sum_{i=0}^N(\\sum_{j=0}^k w_j x^{(i)}_j - y^{(i)})^2 \\\\\n",
    "= \\frac{1}{2N}\\sum_{i=0}^N (h_W(x^{(i)}) - y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(W: np.ndarray, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = solutions.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(X.shape[1])\n",
    "print('your solution:', loss(W, X, Y))\n",
    "print('provided solution:', solutions.loss(W, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and Gradient Steps\n",
    "\n",
    "For every iteration:\n",
    "* calculate partial derivatives of cost function with respect to every element of W:\n",
    "\n",
    "$$\\epsilon_j = \\frac{\\partial}{\\partial w_j}L(W) = \\frac{1}{N} \\sum_{i=1}^N(h_W(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "* **simultaneously** update every element of W:\n",
    "\n",
    "$$w_j = w_j - \\alpha \\epsilon_j$$ \n",
    "\n",
    "Where $\\alpha$ is our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_step(\n",
    "    W: np.ndarray, \n",
    "    X: np.ndarray, \n",
    "    Y: np.ndarray, \n",
    "    learning_rate=0.01\n",
    ") -> np.ndarray:\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradient_step = solutions.gradient_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gradient descent na wielu wartościach\n",
    "* zauważyć, że wartości są z różnych przedziałów => normalizacja\n",
    "* Zauważyć że overfittujemy => train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression: a possible use case\n",
    "\n",
    "This is a plot of a secret polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = solutions.secret_polynomial\n",
    "X = np.arange(-4, 4, 0.7)\n",
    "Y = [secret(x) for x in X]\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What degree of a polynomial could that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_poly_features(X, proposed_degree):\n",
    "    # notice that x ** 0 = 1, so bias feature is already added here\n",
    "    return np.array([[x ** n for n in range(proposed_degree)] for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_degree = 7\n",
    "features = to_poly_features(X, proposed_degree)\n",
    "targets = np.array(Y)\n",
    "for i in range(5):\n",
    "    print(features[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling to the rescue!\n",
    "\n",
    "We want all our features to be roughly in the same range, i.e [-1, 1]. This is called **data normalization**. \n",
    "\n",
    "One way to achieve it is **mean normalization**:\n",
    "\n",
    "$$x_i = \\frac{x_i - \\mu_i}{max(X) - min(X)}$$\n",
    "\n",
    "Of course, since $x_0$ is always equal to 1, we don't normalize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalization(X, means=None, ranges=None):\n",
    "    # implement me!\n",
    "    # X - a matrix of features\n",
    "    # calculate means and ranges if necessary\n",
    "    # calculate normalized matrix X using calculated or given means and ranges\n",
    "    # return X, means and ranges (we may want to reuse them)\n",
    "    # do not normalize the first column of ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_normalization = solutions.mean_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, means, ranges = mean_normalization(features)\n",
    "for i in range(5):\n",
    "    print(features[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(proposed_degree)\n",
    "print(W)\n",
    "costs = []\n",
    "steps = 100000\n",
    "\n",
    "for i in range(steps):\n",
    "    W = gradient_step(W, features, targets, 0.01)\n",
    "    costs.append(loss(W, features, targets))\n",
    "\n",
    "# it is always a good idea to plot the cost function to see how learning goes\n",
    "step_nums = [i for i in range(steps)]\n",
    "plt.scatter(x=step_nums, y=costs)\n",
    "plt.show()\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_targets = hypotheses(W, features)\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X, calculated_targets, color='red')\n",
    "plt.show()\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_X = np.arange(-6, 6, 0.3)\n",
    "more_Y = [secret(x) for x in more_X]\n",
    "more_features = to_poly_features(more_X, proposed_degree)\n",
    "more_features, means, ranges = mean_normalization(more_features, means, ranges)\n",
    "more_targets = np.array(Y)\n",
    "\n",
    "more_calculated_targets = hypotheses(W, more_features)\n",
    "plt.scatter(more_X, more_Y)\n",
    "plt.plot(more_X, more_calculated_targets, color='red')\n",
    "plt.show()\n",
    "W\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(solutions.perform_polynomial_regression,\n",
    "        steps=widgets.IntSlider(min=100,max=1000000,step=1000,value=1000), \n",
    "        degree=widgets.IntSlider(min=1,max=30,step=1,value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's play with real-life data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = np.genfromtxt('houses.csv', delimiter=',')\n",
    "# area, number of bedrooms, price\n",
    "\n",
    "houses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = houses[:, 2]\n",
    "# relationship between area and price\n",
    "areas = houses[:, 0]\n",
    "plt.scatter(areas, prices)\n",
    "plt.show()\n",
    "\n",
    "# relation between no. of bedrooms\n",
    "bedrooms_nos = houses[:, 1]\n",
    "plt.scatter(bedrooms_nos, prices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, data must be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = add_bias_feature(houses[:, :-1]) \n",
    "targets = houses[:, 2]\n",
    "features, _, _ = mean_normalization(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(houses) * 2/3) \n",
    "train_numbers = random.sample(range(len(houses)), train_size)\n",
    "\n",
    "train_features = np.array([features[i] for i in range(len(houses)) if i in train_numbers])\n",
    "train_targets = np.array([targets[i] for i in range(len(houses)) if i in train_numbers])\n",
    "\n",
    "test_features = np.array([features[i] for i in range(len(houses)) if i not in train_numbers])\n",
    "test_targets = np.array([targets[i] for i in range(len(houses)) if i not in train_numbers])\n",
    "\n",
    "len(train_features), len(train_targets), len(test_featW = gradient_step(W, train_features, train_targets, 0.1)ures), len(test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model. Our model will consider only the training se during the training. \n",
    "\n",
    "We will plot the cost function to see how well the model performs on the training data, but also, separately, plot cost calculated for test data. This will help us see how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(3)\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "steps = 1000\n",
    "\n",
    "for i in range(steps):\n",
    "    W = gradient_step(W, train_features, train_targets, 0.1)\n",
    "    train_costs.append(cost(W, train_features, train_targets))\n",
    "    test_costs.append(cost(W, test_features, test_targets))\n",
    "    \n",
    "step_nums = [i for i in range(steps)]\n",
    "# plt.scatter(x=step_nums, y=train_costs)\n",
    "plt.scatter(x=step_nums, y=test_costs, color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(areas, bedrooms_nos, prices)\n",
    "ax.view_init(40, 100)\n",
    "\n",
    "ax.set_xlabel('area')\n",
    "ax.set_ylabel('bedrooms')\n",
    "ax.set_zlabel('price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
