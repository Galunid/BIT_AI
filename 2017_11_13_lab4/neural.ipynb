{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi!\n",
    "\n",
    "Today we're going to finish the topic of logitic regression, and then talk about the mightiest buzzword of them all - **neural networks**. The topic is very complex and you should treat today only as an introduction you can and should follow up on. We're merely scratching the surface!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But first - Logistic regression (again)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick rewind\n",
    "$$h_w(x) = \\sigma(\\sum_{j=0}^k x_j w_j ) = \\sigma(xw)$$\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/+T1fEMnODgdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff51c094f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "x = np.linspace(-10, 10)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we introduced te topic of classification - essentially training our models to decide whether an example 'is' or 'isn't' something. But in real-life, such a binary use case is sometimes *just* not enough.\n",
    "\n",
    "https://www.youtube.com/watch?v=ACmydtFDTGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "To solve the problem of classifying an object as one of multiple classes, we do a one-vs-all prediction. \n",
    "Previously we calculated $h_w(x)$ and applied $sigmoid$ function to it, to calculate the 'probablility' of our example being positive or not. Since $\\hat{y} \\in [0,1]$ We chose a 'threshold' in that range below which we can treat our example as negative and above which - as positive.\n",
    "\n",
    "For multiple classes, we must essentially calculate a hypothesis for **every single one** of possible categories. If hypothesis for a given category is high enough, there is a high probability that our object is of that category. In the other case, it means that it belongs to some other category (but we don't know which one - we need other hypotheses for that). \n",
    "\n",
    "![alt text](img/multiclass.PNG)\n",
    "This is called one-versus-all classification. Ultimately we choose the category whose hypothesis has the highest probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before\n",
    "\n",
    "Until now, a hypothesis $h_w(x^{(i)})$ for a given object $x^{(i)}$ represented as a vector of features $[x_0^{(i)}, x_1^{(i)}, ... x_k^{(i)}]$ was represented by a scalar:\n",
    "\n",
    "$$h_w(x) = \\sigma(\\sum_{j=0}^k w_j x_j) = \\sigma(wx)$$\n",
    "\n",
    "Where w was a vector a weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now\n",
    "\n",
    "If $m$ is the number of possible categories, then for every vector of features we want to perform multiple logistic regressions (for every possible category we might classify it as). \n",
    "Essentially, for every vector of $k$ features we now want to obtain a vector of $m$ hypothesis scalars:\n",
    "\n",
    "$$[x_0^{(i)}, x_1^{(i)}, ... x_k^{(i)}] \\xrightarrow{\\text{classification}} [h_0^{(i)}, h_1^{(i)}, ... h_m^{(i)}]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every logistic regression we need a separate $k$-dimensional vector (or a $k \\times 1$ matrix) of weights. \n",
    "If we want to vectorize our computations, we can merge all of the weights vectors into a single, $k \\times m$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To sum it up\n",
    "- $n$ - number of examples in the dataset (objects we want to classify)\n",
    "- $k$ - number of features every object has\n",
    "- $m$ - number of possible categories to classify as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ - an $n \\times k$ matrix representing the examples\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_0^{(1)} & x_1^{(1)}  &  & ...  &x_k^{(1)}\\\\ \n",
    "x_0^{(2)} &...  &  &...  & \\\\ \n",
    "... &  &  &...  & \\\\ \n",
    "x_0^{(n)} &  & ... &  & x_k^{(n)}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$ - an $k \\times m$ matrix representing weights in logistic regression for every feature in every category\n",
    "\n",
    "\\begin{bmatrix}\n",
    "w_0^{(1)} & w_0^{(2)}  &  & ...  &w_0^{(m)}\\\\ \n",
    "w_1^{(1)} &...  &  &...  & \\\\ \n",
    "... &  &  &...  & \\\\ \n",
    "w_k^{(1)} &  & ... &  & w_k^{(m)}\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_W(X)$ - an $n \\times m$ matrix representing hypothesis vectors for every example and category \n",
    "\n",
    "$$\n",
    "h_W(X) = \\sigma(XW)\n",
    "$$\n",
    "\n",
    "We'll denote j-th hypothesis of i-th example as $$h_w^{(j)}(x^{(i)})$$\n",
    "Computationally-wise, the only thing that changes is the $m$ dimension of W.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As for cost function...\n",
    "\n",
    "$$ L^{(j)}(w) = -\\sum_{i=0}^n y^{(i,j)}\\log{h_w^{(j)}(x^{(i)})} + (1-y^{(i,j)})\\log{(1-h_w(x^{(i)}))}$$\n",
    "\n",
    "We have a vector of cost values for every category $j$, which is useful in updating weights in gradient descent. If we want to plot the cost function, we can sum or count the mean of all those values.\n",
    "\n",
    "Gradient descent also works the same way as before.\n",
    "\n",
    "#### WTF is $y^{(i,j)}$?\n",
    "\n",
    "We can now look at y as a matrix of one-hot values. If $y^{(i,j)} = 1 $, then example $i$ is of class $j$. \n",
    "\n",
    "This also means the rest of values in $y^{(i)}$ are, of course, zeros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-3-ef31fca8001b>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ef31fca8001b>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    def cost(W, X, Y, eps=0.01):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# hint - if you implemented those functions correctly last time, you can just paste your implementations below\n",
    "\n",
    "def hypotheses(W, X):\n",
    "    # implement me!\n",
    "\n",
    "\n",
    "def cost(W, X, Y, eps=0.01):\n",
    "    # implement me!\n",
    "\n",
    "\n",
    "def gradient_step(W, X, Y,learning_rate=0.01):\n",
    "    # implement me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = solutions._hypotheses\n",
    "cost = solutions.cost\n",
    "gradient_step = solutions.gradient_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUriLiISQwl1EJIQU7iIiIaRwFxEJof8Pl5SgQCZa2J8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff51a030710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[1, 0, 0.5, 1], [1, 1, 0, 0], [1, 0, 1, 0]])\n",
    "Y = np.array([[0, 1], [1, 0], [0, 1]])\n",
    "W = np.random.random((4,2))\n",
    "costs = []\n",
    "steps = 10000\n",
    "\n",
    "for i in range(steps):\n",
    "    W = gradient_step(W, X, Y)\n",
    "    costs.append(cost(W, X, Y))\n",
    "\n",
    "plt.plot(np.arange(steps), costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST - something more ambitious\n",
    "\n",
    "MNIST is one of the most famous datasets for beginers in Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist_dir = '/tmp/mnist'\n",
    "mnist = fetch_mldata('MNIST original', data_home=mnist_dir)\n",
    "print(mnist.data.shape)\n",
    "img = mnist.data[0]\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image of a number can be visualized as array of $784 (= 28*28)$ numbers, or jus a picture. \n",
    "For convenience values of pixels are stored not as a 2D array, but as a vector, so in order to be displayed, the vector must be reshaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = img.reshape(28,28) / 255\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to treat every single pixel as a separate feature. In order to do so, let's normalize them. We'll also create one-hot vectors of labels we can fit our model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_count = mnist.data.shape[0]\n",
    "labels = mnist.target.astype(int)\n",
    "normalized_pixels_nobias = mnist.data / 255\n",
    "one_hot_labels = np.zeros((examples_count, 10))\n",
    "one_hot_labels[np.arange(examples_count), labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mnist_elem(index):\n",
    "    img = mnist.data[rand_no]\n",
    "    pixels = img.reshape(28,28) / 255\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()\n",
    "    print('label:', labels[rand_no])\n",
    "    print('label as a one-hot vector:', one_hot_labels[rand_no])\n",
    "\n",
    "examples_count = normalized_pixels_nobias.shape[0]\n",
    "rand_no = np.random.randint(0, examples_count)\n",
    "display_mnist_elem(rand_no)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides normalizing, a bias feature must be added to all examples making the count of features 785.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_pixels = solutions.add_bias_feature(normalized_pixels_nobias) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's randomly pick training data from our dataset. In this example we're going to pick only 10000 examples for the sake of training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_numbers = np.arange(examples_count)\n",
    "np.random.shuffle(rand_numbers)\n",
    "\n",
    "train_count = 10000\n",
    "train_numbers = rand_numbers[:train_count]\n",
    "X_train = np.array([normalized_pixels[i] for i in range(examples_count) if i in train_numbers])\n",
    "Y_train = np.array([one_hot_labels[i] for i in range(examples_count) if i in train_numbers])\n",
    "\n",
    "X_test = np.array([normalized_pixels[i] for i in range(examples_count) if i not in train_numbers])\n",
    "\n",
    "Y_test = np.array([mnist.target[i] for i in range(examples_count) if i not in train_numbers])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we train our model on a limited number of examples ( $10000 = \\frac{1}{7}$ of the dataset) and for a relatively short number of epochs ($1000$). You are more than welcome to re-run this code with different numbers to see how they will affect the accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.random((785,10)) # 784 + bias feature\n",
    "costs = []\n",
    "steps = 1000\n",
    "\n",
    "for i in range(steps):\n",
    "    W = gradient_step(W, X_train, Y_train)\n",
    "    costs.append(cost(W, X_train, Y_train))\n",
    "\n",
    "plt.plot(np.arange(steps), costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_no = np.random.randint(0, examples_count)\n",
    "display_mnist_elem(rand_no)\n",
    "img_pixels = normalized_pixels[rand_no]\n",
    "predicted_H = hypotheses(W, img_pixels)\n",
    "predicted_class = np.argmax(predicted_H)\n",
    "\n",
    "print('predicted hypotheses:', predicted_H)\n",
    "print('predicted_class:', predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test the accuracy on the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_test = hypotheses(W, X_test)\n",
    "predicted_test_labels = np.argmax(H_test, axis=1)\n",
    "accurate = predicted_test_labels == Y_test\n",
    "len([a for a in accurate if a]) / len(accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MNIST it's actually embarassingly bad (best models achieve even 99.9% accuracy), but it's not so bad for one matrix trained on $\\frac{1}{7}$ of the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We promised you neural networks today, though!\n",
    "\n",
    "Now that you understand how logistic regression works, you'll see that at a first glance, neural networks have some similarities to them.\n",
    "\n",
    "## Layers\n",
    "\n",
    "Neural networks are made up of layers. Every layer transorms the data given to it by previous layer in the form of **activation values** and conveys the output to the next one.\n",
    "\n",
    "![alt text](img/neural_schema.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some old/new notations\n",
    "\n",
    "#### activation - values passed as an input to a layer\n",
    "$a^{(j)}$ - activation values in layer $j$ (a vector)\n",
    "\n",
    "#### layer matrices\n",
    "$W^{(j)}$ - matrix of weights mapping activation of layer $j$ into layer $j+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "For a single feature vector (single example) $x$:\n",
    "\n",
    "Activation of the first (input) layer is simply the (normalized) feature vector (including the bias feature!) of our example:\n",
    "\n",
    "$$a^{(1)} = x$$\n",
    "\n",
    "We denote output of the $(j-1)^{th}$ layer as:\n",
    "\n",
    "$$z^{(j)} = a^{(j-1)}W^{(j-1)}$$ (vector $\\times$ matrix, so outputs are vectors).\n",
    "\n",
    "Activation of $j^{th}$ layer can then be calculated as:\n",
    "\n",
    "$$a^{(j)} = g(z^{(j)})$$\n",
    "\n",
    "Where $g$ is an **activation function** - for example sigmoid.\n",
    "\n",
    "**After** calculating $a^{(j)}$ vector, bias feature must be added to it.\n",
    "\n",
    "The final output - our hypothesis - is the output of the final layer with activation function applied to it. So if the network has $L$ layers, the equation is:\n",
    "$$h_W(x) = a^{(L+1)} = g(z^{(L+1)})$$\n",
    "\n",
    "Though this example was described with only one feature vector $x$, it's analogous when there are more examples and vector becomes a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice the similarities with logistic regression?\n",
    "\n",
    "In a way, you could think of it as network learning it's own features. Through adding more layers, we are able to calculate and represent more complex, non-linear models more elegantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, w_1, w_2, w_3):\n",
    "    # assuming X is normalized and contains bias features\n",
    "    # you could of course extend the function to more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_prop = solutions.forward_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But wait! What about cost? Optimization? Learning?\n",
    "\n",
    "Learning happens through **backpropagation** algorithm which is all about propagation of errors through the network and is a bitch to implement in such a short time. You can read more about it here:\n",
    "\n",
    "http://cs231n.github.io/optimization-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
