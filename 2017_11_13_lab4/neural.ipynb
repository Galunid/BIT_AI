{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi!\n",
    "\n",
    "Today we're going to talk about the mightiest buzzword of them all - **neural networks**. The topic is very complex and you should treat today only as an introduction you can and should follow up on. We're merely scratching the surface!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But first - Logistic regression (again)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick rewind\n",
    "$$h_w(x) = \\sigma(\\sum_{j=0}^k x_j w_j ) = \\sigma(xw)$$\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "x = np.linspace(-10, 10)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we introduced te topic of classification - essentially training our models to decide whether an example 'is' or 'isn't' something. But in real-life, such a binary use case is sometimes *just* not enough.\n",
    "\n",
    "https://www.youtube.com/watch?v=ACmydtFDTGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "To solve the problem of classifying an object as one of multiple classes, we do a one-vs-all prediction. \n",
    "Previously we calculated $h_w(x)$ and applied $sigmoid$ function to it, to calculate the 'probablility' of our example being positive or not. Since $\\hat{y} \\in [0,1]$ We chose a 'threshold' in that range below which we can treat our example as negative and above which - as positive.\n",
    "\n",
    "For multiple classes, we must essentially calculate a hypothesis for **every single one** of possible categories. If hypothesis for a given category is high enough, there is a high probability that our object is of that category. In the other case, it means that it belongs to some other category (but we don't know which one - we need other hypotheses for that). \n",
    "\n",
    "![alt text](img/multiclass.PNG)\n",
    "This is called one-versus-all classification. Ultimately we choose the category whose hypothesis has the highest probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before\n",
    "\n",
    "Until now, a hypothesis $h_w(x^{(i)})$ for a given object $x^{(i)}$ represented as a vector of features $[x_0^{(i)}, x_1^{(i)}, ... x_k^{(i)}]$ was represented by a scalar:\n",
    "\n",
    "$$h_w(x) = \\sigma(\\sum_{j=0}^k w_j x_j) = \\sigma(wx)$$\n",
    "\n",
    "Where w was a vector a weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now\n",
    "\n",
    "If $m$ is the number of possible categories, then for every vector of features we want to perform multiple logistic regressions (for every possible category we might classify it as). \n",
    "Essentially, for every vector of $k$ features we now want to obtain a vector of $m$ hypothesis scalars:\n",
    "\n",
    "$$[x_0^{(i)}, x_1^{(i)}, ... x_k^{(i)}] \\xrightarrow{\\text{classification}} [h_0^{(i)}, h_1^{(i)}, ... h_m^{(i)}]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every logistic regression we need a separate $k$-dimensional vector (or a $k \\times 1$ matrix) of weights. \n",
    "If we want to vectorize our computations, we can merge all of the weights vectors into a single, $k \\times m$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To sum it up\n",
    "- $n$ - number of examples in the dataset (objects we want to classify)\n",
    "- $k$ - number of features every object has\n",
    "- $m$ - number of possible categories to classify as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ - an $n \\times k$ matrix representing the examples\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_0^{(1)} & x_1^{(1)}  &  & ...  &x_k^{(1)}\\\\ \n",
    "x_0^{(2)} &...  &  &...  & \\\\ \n",
    "... &  &  &...  & \\\\ \n",
    "x_0^{(n)} &  & ... &  & x_k^{(n)}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$ - an $k \\times m$ matrix representing weights in logistic regression for every feature in every category\n",
    "\n",
    "\\begin{bmatrix}\n",
    "w_0^{(1)} & w_0^{(2)}  &  & ...  &w_0^{(m)}\\\\ \n",
    "w_1^{(1)} &...  &  &...  & \\\\ \n",
    "... &  &  &...  & \\\\ \n",
    "w_k^{(1)} &  & ... &  & w_k^{(m)}\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_W(X)$ - an $n \\times m$ matrix representing hypothesis vectors for every example and category \n",
    "\n",
    "$$\n",
    "h_W(X) = \\sigma(XW)\n",
    "$$\n",
    "\n",
    "We'll denote j-th hypothesis of i-th example as $$h_w^{(j)}(x^{(i)})$$\n",
    "Computationally-wise, the only thing that changes is the $m$ dimension of W.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As for cost function...\n",
    "\n",
    "$$ L^{(j)}(w) = -\\sum_{i=0}^n y^{(i,j)}\\log{h_w^{(j)}(x^{(i)})} + (1-y^{(i,j)})\\log{(1-h_w(x^{(i)}))}$$\n",
    "\n",
    "We have a vector of cost values for every category $j$, which is useful in updating weights in gradient descent. If we want to plot the cost function, we can sum or count the mean of all those values.\n",
    "\n",
    "Gradient descent also works the same way as before.\n",
    "\n",
    "#### WTF is $y^{(i,j)}$?\n",
    "\n",
    "We can now look at y as a matrix of one-hot values. If $y^{(i,j)} = 1 $, then example $i$ is of class $j$. \n",
    "\n",
    "This also means the rest of values in $y^{(i)}$ are, of course, zeros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint - if you implemented those functions correctly last time, you can just paste your implementations below\n",
    "\n",
    "def hypotheses(W, X):\n",
    "    # implement me!\n",
    "\n",
    "\n",
    "def cost(W, X, Y, eps=0.01):\n",
    "    # implement me!\n",
    "\n",
    "\n",
    "def gradient_step(W, X, Y,learning_rate=0.01):\n",
    "    # implement me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = solutions._hypotheses\n",
    "cost = solutions.cost\n",
    "gradient_step = solutions.gradient_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 0, 0.5, 1], [1, 1, 0, 0], [1, 0, 1, 0]])\n",
    "Y = np.array([[0, 1], [1, 0], [0, 1]])\n",
    "W = np.random.random((4,2))\n",
    "costs = []\n",
    "steps = 10000\n",
    "\n",
    "for i in range(steps):\n",
    "    W = gradient_step(W, X, Y)\n",
    "    costs.append(cost(W, X, Y))\n",
    "\n",
    "plt.plot(np.arange(steps), costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST - something more ambitious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist_dir = '/tmp/mnist'\n",
    "mnist = fetch_mldata('MNIST original', data_home=mnist_dir)\n",
    "print(mnist.data.shape)\n",
    "img = mnist.data[0]\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = img.reshape(28,28) / 255\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_count = mnist.data.shape[0]\n",
    "labels = mnist.target.astype(int)\n",
    "normalized_pixels = mnist.data / 255\n",
    "one_hot_labels = np.zeros((examples_count, 10))\n",
    "one_hot_labels[np.arange(examples_count), labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mnist_elem(index):\n",
    "    img = mnist.data[rand_no]\n",
    "    pixels = img.reshape(28,28) / 255\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()\n",
    "    print('label:', labels[rand_no])\n",
    "    print('label as one-hot vector:', one_hot_labels[rand_no])\n",
    "\n",
    "examples_count = normalized_pixels.shape[0]\n",
    "rand_no = np.random.randint(0, examples_count)\n",
    "display_mnist_elem(rand_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add a bias feature to all examples making the count of features 785\n",
    "normalized_pixels = solutions.add_bias_feature(normalized_pixels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_numbers = np.arange(examples_count)\n",
    "np.random.shuffle(rand_numbers)\n",
    "\n",
    "train_numbers = rand_numbers[:10000]\n",
    "X_train = np.array([normalized_pixels[i] for i in range(examples_count) if i in train_numbers])\n",
    "Y_train = np.array([one_hot_labels[i] for i in range(examples_count) if i in train_numbers])\n",
    "\n",
    "X_test = np.array([normalized_pixels[i] for i in range(examples_count) if i not in train_numbers])\n",
    "Y_test = np.array([one_hot_labels[i] for i in range(examples_count) if i not in train_numbers])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.random((785,10)) # 784 + bias feature\n",
    "costs = []\n",
    "steps = 1000\n",
    "\n",
    "for i in range(steps):\n",
    "#     print(i)\n",
    "    W = gradient_step(W, X_train, Y_train)\n",
    "    costs.append(cost(W, X_train, Y_train))\n",
    "\n",
    "plt.plot(np.arange(steps), costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_no = np.random.randint(0, examples_count)\n",
    "display_mnist_elem(rand_no)\n",
    "img_pixels = normalized_pixels[rand_no]\n",
    "predicted_H = hypotheses(W, img_pixels)\n",
    "predicted_class = np.argmax(predicted_H)\n",
    "\n",
    "print('predicted hypotheses:', predicted_H)\n",
    "print('predicted_class:', predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
