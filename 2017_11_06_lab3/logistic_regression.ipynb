{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's workshop we are going to learn about most known concept of supervised learning which is `classification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(load_breast_cancer().DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, train_size=0.7)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, train_size=0.66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What is classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a problem of predicting discrete value (classes) for given features. It is mainly viewed as a supervised learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What about applying linear regression for classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret these predictions? Maybe we need something different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What is logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is about applying \"squashing\" function to the hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = h_w(x)$$ \n",
    "\n",
    "$$h_w(x) = \\sum_{j=0}^k w_j x_j = wx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = \\sigma(h_w(x))$$ \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, sigmoid(x))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(np.inf), sigmoid(-np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about loss? Is MSE still applicable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are reasons why we are not using MSE. Instead we use log-loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(w) = -\\sum_{i=0}^n y^{(i)}\\log{h_w(x^{(i)})} + (1-y^{(i)})\\log{(1-h_w(x^{(i)}))}$$\n",
    "\n",
    "$$ y^{(i)} \\in \\{0, 1\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 1\n",
    "\n",
    "x = np.linspace(0.0001, 1, 1000)\n",
    "plt.plot(x, -np.log(x))\n",
    "plt.ylim(-1, 10)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 0\n",
    "\n",
    "x = np.linspace(0, 0.9999, 1000)\n",
    "plt.plot(x, -np.log(1 - x))\n",
    "plt.ylim(-1, 10)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about gradient descent procedure? How does it change? Let's derive gradient on blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions = reload(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_feature(X):\n",
    "       return np.c_[np.ones(len(X)), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = add_bias_feature(X_train)\n",
    "X_val = add_bias_feature(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, *norm_parameters = solutions.std_normalization(X_train)\n",
    "X_val, *_ = solutions.std_normalization(X_val, *norm_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.zeros(X_train.shape[1])\n",
    "train_costs = []\n",
    "val_costs = []\n",
    "train_steps = 100000\n",
    "for _ in range(train_steps):\n",
    "    train_costs.append(solutions.cost(W, X_train, y_train, eps=0.001))\n",
    "    val_costs.append(solutions.cost(W, X_val, y_val, eps=0.001))\n",
    "    W = solutions.gradient_step(W, X_train, y_train, learning_rate=0.01)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(train_steps), train_costs)\n",
    "plt.plot(np.arange(train_steps), val_costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(solutions._hypotheses(W, X_train) >= 0.5, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression(C=10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with overfitting?\n",
    "\n",
    "**Regularization to the rescue!**\n",
    "\n",
    "In Logistic (as well as Linear) Regression we can make sure that elements of the weights vector don't grow too dramatically. We do this by penalizing their size additionally in the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularized cost function\n",
    "\n",
    "$$\n",
    "L_{reg}(W) = L(W) + \\frac{\\lambda}{2n}\\sum_{j=1}^k w_j^2\n",
    "$$\n",
    "\n",
    "...whatever the original cost function was!\n",
    "\n",
    "**We don't regularize the first weight element, which is responsible for bias!**\n",
    "\n",
    "$\\lambda$ parameter is, just like learning rate, something that is best calculated empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_reg(W, X, Y, l=0.1):\n",
    "    # implement me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_reg = solutions.cost_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularized gradient descent\n",
    "\n",
    "The only thing that changes is the partial derivative of cost for all $j \\in [1.. k]$ (since we don't regularize $w_0$)\n",
    "\n",
    "$$\\epsilon_0 = \\frac{\\partial}{\\partial w_j}L(W) = \\frac{1}{N} \\sum_{i=1}^N(h_W(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "$$\\epsilon_j = \\frac{\\partial}{\\partial w_j}L(W) = \\frac{1}{N} \\sum_{i=1}^N(h_W(x^{(i)}) - y^{(i)})x_j^{(i)} + \\frac{\\lambda}{n}w_j$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step_reg(W, X, Y, l=0.1):\n",
    "    #implement me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_step_reg = solutions.gradient_step_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to measure performance of our model?\n",
    "#### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide classifications of our model into four classes:\n",
    "\n",
    "| Predicted/Actual | 0   | 1   |\n",
    "|------------------|-----|-----|\n",
    "| 0                | True negative | False negative|\n",
    "| 1                | False positive | True positive | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy - a first intuition**\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{T_p + T_n}{total}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(actual_predictions, model_predictions):\n",
    "    # implement me!\n",
    "    # both arguments are np.arrays of zeros and ones symbolizing \n",
    "    # results of classification for each exampleb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = solutions.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What problems do you see with such a metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out there is a more reliable way to measure the performance of our model:\n",
    "\n",
    "- **Precision** - *what fraction of our positive classifications is correct?*\n",
    "$$\n",
    "Precision = \\frac{T_p}{T_p + F_p}\n",
    "$$\n",
    "\n",
    "- **Recall** - *what fraction of actual positive examples has been classified correctly?*\n",
    "$$\n",
    "Recall = \\frac{T_p}{T_p + F_n}\n",
    "$$\n",
    "\n",
    "We want both of those values to be as high as possible (duh).\n",
    "However, sometimes we have to make a trade off between them and decide with our classification method that one will be higher and the other lower.\n",
    "\n",
    "###### Can you think of any simple ways to increase one of those metrics? (without changing the model or the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the possible metrics which takes those two into account is the **F score**, which will be high if both precision and recall are high, but low if we sacrifice precision to increase recall or the other way around.\n",
    "\n",
    "$$\n",
    "F score = \\frac{2PR}{P + R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(actual_predictions, model_predictions):\n",
    "    # implement me!\n",
    "    # both arguments are np.arrays of zeros and ones symbolizing \n",
    "    # results of classification for each example\n",
    "    \n",
    "def recall(actual_predictions, model_predictions):\n",
    "    # implement me!\n",
    "    # both arguments are np.arrays of zeros and ones symbolizing \n",
    "    # results of classification for each example\n",
    "    \n",
    "def f_score(actual_predictions, model_predictions):\n",
    "    # implement me!\n",
    "    # both arguments are np.arrays of zeros and ones symbolizing \n",
    "    # results of classification for each example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUROC Curve \n",
    "Another way to visualize the performance of our model is to plot \n",
    "\n",
    "**A**rea\n",
    "\n",
    "**U**nder\n",
    "\n",
    "**R**eceiver\n",
    "\n",
    "**O**perating\n",
    "\n",
    "**C**haracteristic\n",
    "\n",
    "curve.\n",
    "\n",
    "This curve indicates the relation between two metrics:\n",
    "\n",
    "- **True positive rate (TPR)** (which is another name for recall)\n",
    "\n",
    "*what fraction of actual positive examples has been classified correctly?*\n",
    "\n",
    "$$\n",
    "TPR = \\frac{T_p}{T_p + F_n} = Recall\n",
    "$$\n",
    "\n",
    "- **False positive rate (FPR)**\n",
    "\n",
    "\n",
    "*what fraction of actual negative examples has been classified incorrectly?*\n",
    "$$\n",
    "FPR = \\frac{F_p}{F_p + T_n}\n",
    "$$\n",
    "\n",
    "The metrics should be calculated for different thresholds of classification in the classifier and then plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = recall\n",
    "\n",
    "def fpr(actual_predictions, model_predictions):\n",
    "    # implement me!\n",
    "    # both arguments are np.arrays of zeros and ones symbolizing \n",
    "    # results of classification for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = solutions.fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0,1, 0.02)\n",
    "\n",
    "classifications_for_thresholds = [model.classify(features, t) for t in thresholds]\n",
    "\n",
    "tprs = [tpr(actual_predictions, model_predictions) for model_predictions in classifications_for_thresholds]\n",
    "\n",
    "fprs = [fpr(actual_predictions, model_predictions) for model_predictions in classifications_for_thresholds]\n",
    "\n",
    "plt.plot(fprs, tprs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
