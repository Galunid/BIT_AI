{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets \n",
    "\n",
    "from load_cifar import load_cifar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a spritual successor of Torch and is being implemented by Facebook. \n",
    "\n",
    "It operates on various levels of abstraction:\n",
    "\n",
    "* Tensor - something similar to `np.array` but can be stored on the GPU\n",
    "* Variable - a part of a computational graph. Holds tensors as the value of the variable, as well as variable's gradients.\n",
    "* Module - a neural network layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by creating a simple, shallow model which we'll use to classify the Iris dataset ( https://archive.ics.uci.edu/ml/datasets/iris )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's load data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris['data']\n",
    "iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Variable(torch.FloatTensor(iris['data']), requires_grad=False)\n",
    "y = Variable(torch.LongTensor(iris['target']), requires_grad=False)\n",
    "# We'll train on the whole dataset - don't ever do that - but for ilustrating behaviour it's good enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of an autograd function - you can use them to define your own operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a helper function to measure accuracy\n",
    "def accuracy(logits, y):\n",
    "    return (logits == y).sum() / y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "relu = MyReLU.apply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether variables are stored on CPU or GPU is decided by their type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "gpu_dtype = dtype = torch.cuda.FloatTensor\n",
    "cpu_dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch works on dynamic computational graphs. It means that with every operation, the graph is constructed from scratch. It's slower, but allows for nice things such as loops.\n",
    "\n",
    "The downside is that models don't infer dimensionality that easily. It can be a pain, especially when building more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_in, H, D_out = 4, 10, 3\n",
    "\n",
    "X_t = X.type(dtype)\n",
    "y_t = y.type(dtype)\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = relu(X_t @ w1) @ w2\n",
    "    \n",
    "    loss = loss_fn(y_pred, y_t.long())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Backward pass\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    # Manually zero the gradients after updating weights\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "    \n",
    "    #\n",
    "    logits = torch.topk(y_pred, 1)[1].data.cpu().numpy().flatten()  \n",
    "    acc = accuracy(logits, y_t.data.cpu().numpy())\n",
    "    if t % 50 == 0: print(t, loss.data[0], acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train a network on a more serious dataset - CIFAR-10 !\n",
    "\n",
    "But first, let's load (already normalized!) data. PyTorch has utilities for that as well. How cool is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_cifar()\n",
    "\n",
    "X_train = X_train.reshape(-1, 32, 32, 3)\n",
    "X_test = X_test.reshape(-1, 32, 32, 3)\n",
    "\n",
    "# in PyTorch, the 'channels' dimension is of higher order than height and width\n",
    "# as a consequence, images have now shapes [?, 3, 32, 32]\n",
    "X_train = X_train.transpose(0, 3, 1, 2)\n",
    "X_test = X_test.transpose(0, 3, 1, 2)\n",
    "\n",
    "# print(X_train[0])\n",
    "X_train = Variable(torch.FloatTensor(X_train), requires_grad=False)\n",
    "y_train = Variable(torch.LongTensor(y_train), requires_grad=False)\n",
    "X_test = Variable(torch.FloatTensor(X_test), requires_grad=False)\n",
    "y_test = Variable(torch.LongTensor(y_test), requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(n=128, X=X_train, y=y_train, dtype=dtype):\n",
    "    ind = np.random.randint(low=0, high=X.size()[0], size=n)\n",
    "    X_np = X.data.numpy()[ind]\n",
    "    y_np = y.data.numpy()[ind]\n",
    "    X_var = Variable(torch.FloatTensor(X_np)).type(dtype)\n",
    "    y_var = Variable(torch.LongTensor(y_np)).type(dtype).long()\n",
    "    return X_var, y_var\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's inside the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_cifar(i):\n",
    "    print(i)\n",
    "    x = X_train.data[i].cpu().numpy().transpose(1, 2, 0)\n",
    "    plt.imshow(x)\n",
    "    plt.show()\n",
    "\n",
    "interact(show_cifar, i=ipywidgets.IntSlider(min=0, max=63))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch you can not only use pre-implemented modules - you can also implement your own. The only thing to do is implement the forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is an easy way to define a model:\n",
    "\n",
    "Note that you could also create non-sequential connections (like Inception layers) for example by implementing your own modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_base = nn.Sequential( \n",
    "    nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.PReLU(),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.PReLU(),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "    nn.PReLU(),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.Dropout(p=0.3),\n",
    "    \n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.PReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.PReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "    nn.PReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.Dropout(p=0.3),\n",
    "    \n",
    "    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "    nn.PReLU(),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "    nn.PReLU(),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "    nn.PReLU(),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.Dropout(p=0.3),\n",
    "    Flatten(),\n",
    "    nn.Linear(2048, 10),  \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train the model, let's see how fast it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy \n",
    "model_cpu = model_base.type(cpu_dtype)\n",
    "model_gpu = copy.deepcopy(model_base).type(gpu_dtype)\n",
    "x = torch.randn(64, 3, 32, 32).type(cpu_dtype)\n",
    "x_var = Variable(x.type(cpu_dtype)) \n",
    "x_gpu = torch.randn(64, 3, 32, 32).type(gpu_dtype)\n",
    "x_var_gpu = Variable(x.type(gpu_dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "ans = model_cpu(x_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "torch.cuda.synchronize() # Make sure there are no pending GPU computations\n",
    "ans = model_gpu(x_var_gpu)        # Feed it through the model! \n",
    "torch.cuda.synchronize() # Make sure there are no pending GPU computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have fun with the model and train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dtype == gpu_dtype: torch.cuda.random.manual_seed(2137)\n",
    "else: torch.random.manual_seed(2137)\n",
    "\n",
    "model = model_base.type(dtype)\n",
    "model.apply(reset)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print('EPOCH', i)\n",
    "    for e in range(1000):\n",
    "        model.train()\n",
    "        x_var, y_var = random_batch()\n",
    "        y_pred = model(x_var)\n",
    "        logits = torch.topk(y_pred, 1)[1].data.cpu().numpy().flatten()    \n",
    "        loss = loss_fn(y_pred, y_var.long())\n",
    "        if (e % 100) == 0: print(e, loss.data[0], accuracy(logits, y_var.data.cpu().numpy()))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_var, y_var = random_batch(X=X_test, y=y_test, dtype=gpu_dtype)\n",
    "y_pred = model(x_var)\n",
    "logits = torch.topk(y_pred, 1)[1].data.cpu().numpy().flatten()\n",
    "acc = accuracy(logits, y_var.data.cpu().numpy())\n",
    "print('accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
