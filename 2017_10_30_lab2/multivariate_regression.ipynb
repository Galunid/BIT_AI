{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "\n",
    "Today we're going to present last-time's topic - **Linear Regression** in a more generalized way. You'll see that this algorithm doesn't change a lot, when we want to fit our model to more features than just one!\n",
    "\n",
    "This will be an occasion to introduce and tackle some problems faced by data scientists on a daily basis, such as:\n",
    "- data normalization\n",
    "- regularization of the cost function\n",
    "- overfitting\n",
    "- dividing the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "import solutions\n",
    "import csv\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "## Previously\n",
    "\n",
    "### $$\\hat{y} = h_W(x) = w_0 + w_1x$$ \n",
    "\n",
    "## Today\n",
    "\n",
    "### $$\\hat{y} = h_W(x_1, x_2, ..., x_k) = w_0 + w_1x_1+ w_2x_2+ w_3x_3+ ... + w_kx_k = w_0 + \\sum_{i=1}^k w_i x_i$$ \n",
    "\n",
    "## or:\n",
    "\n",
    "### $W$ - vector of coefficients or 'weights'\n",
    "\n",
    "$$ W = [w_0, w_1, ..., w_k]$$\n",
    "\n",
    "### $X^{(i)}$ - vector of  features of an i-th test case\n",
    "$$ X^{(i)} = [x^{(i)}_0, x^{(i)}_1, ... x^{(i)}_k] $$\n",
    "... where $x_0$ = 1, so that\n",
    "$$h_W(X^{(i)}) = \\sum_{j=0}^k w_j x_j = W * X^{(i)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_feature(X):\n",
    "       return np.c_[np.ones(len(X)), X]\n",
    "\n",
    "X = np.array([[1,2,3], [4,5,6]])\n",
    "print(X)\n",
    "print(add_bias_feature(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypotheses(W, X):\n",
    "    # W: a vector of weights\n",
    "    # X: a list of feature vectors of some objects (so effectively a matrix)\n",
    "    # return a vector of hypotheses for *all* x-s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = solutions.hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function\n",
    "\n",
    "### $$L = \\frac{1}{2N}\\sum_{i=0}^N(h_W(x^{(i)}) - y^{(i)})^2 $$\n",
    "## Previously\n",
    "\n",
    "### $$L(w_0, w_1) = \\frac{1}{2N}\\sum_{i=0}^N(w_0 + w_1x^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "## Today\n",
    "\n",
    "### $$L(w_0, w_1, ... w_n) = L(W) = \\frac{1}{2N}\\sum_{i=0}^N(\\sum_{j=0}^k w_j x^{(i)}_j - y^{(i)})^2 = \\frac{1}{2N}\\sum_{i=0}^N (h_W(x^{(i)}) - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(W, X, Y):\n",
    "    # W: a vector of weights\n",
    "    # X: a list of feature vectors of some objects (so effectively a matrix)\n",
    "    # Y: a vector of our values\n",
    "    # return cost (a scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = solutions.cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "For every iteration:\n",
    "* calculate partial derivatives of cost function with respect to every element of W:\n",
    "\n",
    "$$\\epsilon_j = \\frac{\\partial}{\\partial w_j}L(W) = \\frac{1}{N} \\sum_{i=1}^N(h_W(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "* **simultaneously** update every element of W:\n",
    "\n",
    "$$w_j = w_j - \\alpha \\epsilon_j$$ \n",
    "\n",
    "Where $\\alpha$ is our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(W, X, Y, learning_rate=0.01):\n",
    "    # W: a vector of weights\n",
    "    # X: a list of feature vectors of some objects (so effectively a matrix)\n",
    "    # Y: a vector of our values\n",
    "    # return a vector of new values of W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_step = solutions.gradient_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression: a possible use case\n",
    "\n",
    "This is a plot of a secret polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = solutions.secret_polynomial\n",
    "X = np.arange(-4, 4, 0.7)\n",
    "Y = [secret(x) for x in X]\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What degree of a polynomial could that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_poly_features(X, proposed_degree):\n",
    "    # notice that x ** 0 = 1, so bias feature is already added here\n",
    "    return np.array([[x ** n for n in range(proposed_degree)] for x in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_degree = 4\n",
    "features = to_poly_features(X, proposed_degree)\n",
    "targets = np.array(Y)\n",
    "features[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling to the rescue!\n",
    "\n",
    "We want all our features to be roughly in the same range, i.e [-1, 1]. This is called **data normalization**. \n",
    "\n",
    "One way to achieve it is **mean normalization**:\n",
    "\n",
    "$$x_i = \\frac{x_i - \\mu_i}{max(X) - min(X)}$$\n",
    "\n",
    "Of course, since $x_0$ is always equal to 1, we don't normalize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalization(X, means=None, ranges=None):\n",
    "    # implement me!\n",
    "    # X - a matrix of features\n",
    "    # calculate means and ranges if necessary\n",
    "    # calculate normalized matrix X using calculated or given means and ranges\n",
    "    # return X, means and ranges (we may want to reuse them)\n",
    "    # do not normalize the first column of ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_normalization = solutions.mean_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, means, ranges = mean_normalization(features)\n",
    "features[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(proposed_degree)\n",
    "costs = []\n",
    "steps = 10000\n",
    "\n",
    "for i in range(steps):\n",
    "    W = gradient_step(W, features, targets, 0.01)\n",
    "    costs.append(cost(W, features, targets))\n",
    "\n",
    "# it is always a good idea to plot the cost function to see how learning goes\n",
    "step_nums = [i for i in range(steps)]\n",
    "plt.scatter(x=step_nums, y=costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_targets = hypotheses(W, features)\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X, calculated_targets, color='red')\n",
    "plt.show()\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_X = np.arange(-6, 6, 0.3)\n",
    "more_Y = [secret(x) for x in more_X]\n",
    "more_features = to_poly_features(more_X, proposed_degree)\n",
    "more_features, means, ranges = mean_normalization(more_features, means, ranges)\n",
    "more_targets = np.array(Y)\n",
    "\n",
    "more_calculated_targets = hypotheses(W, more_features)\n",
    "plt.scatter(more_X, more_Y)\n",
    "plt.plot(more_X, more_calculated_targets, color='red')\n",
    "plt.show()\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(solutions.perform_polynomial_regression,\n",
    "        steps=widgets.IntSlider(min=100,max=1000000,step=1000,value=1000), \n",
    "        degree=widgets.IntSlider(min=1,max=30,step=1,value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's play with real-life data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = np.genfromtxt('houses.csv', delimiter=',')\n",
    "# area, number of bedrooms, price\n",
    "\n",
    "houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = houses[:, 2]\n",
    "# relationship between area and price\n",
    "areas = houses[:, 0]\n",
    "plt.scatter(areas, prices)\n",
    "plt.show()\n",
    "\n",
    "# relation between no. of bedrooms\n",
    "bedrooms_nos = houses[:, 1]\n",
    "plt.scatter(bedrooms_nos, prices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, data must be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = add_bias_feature(houses[:, :-1]) \n",
    "targets = houses[:, 2]\n",
    "features, _, _ = mean_normalization(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(houses) * 2/3) \n",
    "train_numbers = random.sample(range(len(houses)), train_size)\n",
    "\n",
    "train_features = np.array([features[i] for i in range(len(houses)) if i in train_numbers])\n",
    "train_targets = np.array([targets[i] for i in range(len(houses)) if i in train_numbers])\n",
    "\n",
    "test_features = np.array([features[i] for i in range(len(houses)) if i not in train_numbers])\n",
    "test_targets = np.array([targets[i] for i in range(len(houses)) if i not in train_numbers])\n",
    "\n",
    "len(train_features), len(train_targets), len(test_features), len(test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model. Our model will consider only the training se during the training. \n",
    "\n",
    "We will plot the cost function to see how well the model performs on the training data, but also, separately, plot cost calculated for test data. This will help us see how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(3)\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "steps = 1000\n",
    "\n",
    "for i in range(steps):\n",
    "    W = gradient_step(W, train_features, train_targets, 0.1)\n",
    "    train_costs.append(cost(W, train_features, train_targets))\n",
    "    test_costs.append(cost(W, test_features, test_targets))\n",
    "    \n",
    "step_nums = [i for i in range(steps)]\n",
    "plt.scatter(x=step_nums, y=train_costs)\n",
    "plt.scatter(x=step_nums, y=test_costs, color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(areas, bedrooms_nos, prices)\n",
    "ax.view_init(40, 100)\n",
    "\n",
    "ax.set_xlabel('area')\n",
    "ax.set_ylabel('bedrooms')\n",
    "ax.set_zlabel('price')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
